{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate CO2 emissions for CMIP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "co2file = '/work/mh0033/m300681/IPCC/processed_data/obs/Global_Carbon_Budget_2018v1.0.xlsx' # where observed data is stored\n",
    "co2path = '/work/mh0033/m300681/IPCC/processed_data/emissions/' # where scenario data is stored, and there data is saved\n",
    "\n",
    "# read in the historical co2 from the Global Carbon Budget\n",
    "xls = pd.ExcelFile(co2file)\n",
    "data = xls.parse(-1)\n",
    "histyears = np.asarray(data['Unnamed: 0'])[14::]\n",
    "fossil = np.asarray(data['Historical CO2 budget'][14::])\n",
    "landuse = np.asarray(data['Unnamed: 2'])[14::]\n",
    "fossil[~(fossil>0)] = 0\n",
    "landuse[~(landuse>0)] = 0\n",
    "histemis = (fossil+landuse)*3.66\n",
    "\n",
    "\n",
    "co2 = {}\n",
    "for scenario in ['rcp26','rcp45','rcp85']:\n",
    "    data = np.genfromtxt(co2path+scenario.upper()+'_EMISSIONS.DAT',skip_header = 38,delimiter = ',')\n",
    "    time = data[:,0]\n",
    "    data = (data[:,1] + data[:,2])*3.66\n",
    "    \n",
    "    rcpyears = time[np.isin(time,np.arange(2018,2101))]\n",
    "    rcpemis = data[np.isin(time,np.arange(2018,2101))]\n",
    "    emis = np.concatenate([histemis[histyears<2018],rcpemis]).astype(float)\n",
    "    years = np.concatenate([histyears[histyears<2018],rcpyears]).astype(float)\n",
    "\n",
    "    cumul = np.nancumsum(emis)\n",
    "   \n",
    "    co2[scenario] = np.asarray([years,cumul]) \n",
    "\n",
    "co2['historical'] = np.asarray([histyears,np.nancumsum(histemis)]).astype(float)\n",
    "\n",
    "with open(co2path+'CO2_CMIP5.npy', 'wb') as file:\n",
    "    pickle.dump(co2,file)\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate CO2 emissions for CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from cdo import Cdo\n",
    "import datetime\n",
    "from netCDF4 import Dataset,num2date\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.interpolate import interp1d\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "from scipy.io import savemat,loadmat\n",
    "import pickle\n",
    "\n",
    "co2path = '/work/mh0033/m300681/IPCC/processed_data/emissions/' # where data is saved\n",
    "co2file = '/work/mh0033/m300681/IPCC/processed_data/obs/Global_Carbon_Budget_2018v1.0.xlsx'\n",
    "\n",
    "# co2 observations (global carbon budget 2018)\n",
    "xls = pd.ExcelFile(co2file)\n",
    "data = xls.parse(-1)\n",
    "histyears = np.asarray(data['Unnamed: 0'][14::])\n",
    "fossil = np.asarray(data['Historical CO2 budget'][14::])\n",
    "landuse = np.asarray(data['Unnamed: 2'])[14::]\n",
    "fossil[~(fossil>0)] = 0\n",
    "landuse[~(landuse>0)] = 0\n",
    "histemis = (fossil+landuse)*3.66\n",
    "\n",
    "# ssp emissions from https://tntcat.iiasa.ac.at/SspDb/dsd?Action=htmlpage&page=welcome\n",
    "sspyears = [2020,2030,2040,2050,2060,2070,2080,2090,2100]\n",
    "ssp = {'ssp119': [39693.726,22847.271,10475.089,2050.362,-1525.978,-4476.970,-7308.783,-10565.023,-13889.788],\n",
    "      'ssp245': [40647.530,43476.063,44252.900,43462.190,40196.485,35235.434,26838.373,16324.392,9682.859],\n",
    "      'ssp126': [39804.013,34734.424,26509.183,17963.539,10527.979,4476.328,-3285.043,-8385.183,-8617.786],\n",
    "      'ssp585': [43712.349,55296.583,68775.698,83298.220,100338.606,116805.249,129647.035,130576.239,126287.310]}\n",
    "\n",
    "co2data = {}\n",
    "for scenario in ['ssp119','ssp126','ssp245','ssp585']:\n",
    "    data = np.concatenate([histemis,np.asarray(ssp[scenario])/1000]).astype(float)\n",
    "    years = np.concatenate([histyears,sspyears]).astype(float)\n",
    "\n",
    "    ## interpolate to yearly\n",
    "    goodyears = np.arange(1750,2101)\n",
    "    emis = interp1d(years,data)(goodyears)\n",
    "\n",
    "    cumul = np.nancumsum(emis)\n",
    "    co2data[scenario] = np.asarray([goodyears,cumul])\n",
    "\n",
    "co2data['historical'] = np.asarray([histyears,np.nancumsum(histemis)]).astype(float)\n",
    "\n",
    "with open(co2path+'CO2_CMIP6.npy','wb') as myFile:\n",
    "    pickle.dump(co2data,myFile)\n",
    "    myFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download CMIP6 model data from ESGF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from cdo import Cdo\n",
    "import sys\n",
    "import pickle\n",
    "from cmip6_download import cmip6_download\n",
    "\n",
    "modellist = ['EC-Earth3','EC-Earth3-Veg','MIROC-ES2L','NorCPM1','NorESM2-LM','GFDL-ESM4','E3SM-1-0','FGOALS-g3','FGOALS-f3-L'] # NESM3\n",
    "\n",
    "modellist = ['CESM2-FV2']\n",
    "variables = ['siarean','sivoln','siareas','sivols','siconc','sivol']\n",
    "\n",
    "scenarios = ['historical','ssp119','ssp126','ssp245','ssp585','piControl']\n",
    "#scenarios = ['historical']\n",
    "for model in modellist:\n",
    "    print(model)\n",
    "    for scenario in scenarios:\n",
    "        print(scenario)\n",
    "        if scenario == 'piControl':\n",
    "            varis = variables#['siareas','siarean']\n",
    "        else:\n",
    "            varis = variables\n",
    "        for var in varis:\n",
    "            cmip6_download(var,'mon',scenario,model)\n",
    "\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data from ESGF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from cdo import Cdo\n",
    "import sys\n",
    "import pickle\n",
    "from cmip6_download import cmip6_download\n",
    "\n",
    "cdo = Cdo()\n",
    "hemisphere = 'sh'\n",
    "if hemisphere == 'nh':\n",
    "    lonlatbox = '0,360,0,90'\n",
    "else:\n",
    "    lonlatbox = '0,360,-90,0'\n",
    "inpath = '/work/mh0033/m300681/IPCC/processed_data/CMIP6/external/download/'\n",
    "outpath = '/work/mh0033/m300681/IPCC/processed_data/CMIP6/'\n",
    "\n",
    "folders = [os.path.basename(x) for x in glob.glob(inpath+'*')]\n",
    "print(folders)\n",
    "\n",
    "# one folder is for one model and one variable, and thus contains\n",
    "# several members of the model\n",
    "for folder in folders:\n",
    "    print(folder)\n",
    "    files = [os.path.basename(x) for x in glob.glob(inpath+folder+'/*.nc')]\n",
    "    # use first file to pull information\n",
    "    variable = files[0].split('_')[0]\n",
    "    scenario = files[0].split('_')[3]\n",
    "    model = files[0].split('_')[2]\n",
    "    realm = files[0].split('_')[1]\n",
    "    gridtype = files[0].split('_')[5]\n",
    "    print(model, variable, scenario)\n",
    "    # figure out the members present\n",
    "    members = list(set([file.split('_')[4] for file in files]))\n",
    "    print(members)\n",
    "    for member in members:\n",
    "        try:\n",
    "            name = variable+'_'+realm+'_'+model+'_'+scenario+'_'+member+'_'+gridtype+'_'\n",
    "            if variable == 'tas':\n",
    "                #gmst\n",
    "                outputfile = outpath+'gmst/gmst_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                if not os.path.isfile(outputfile):\n",
    "                    cdo.fldmean(input='-chname,tas,gmst -selvar,tas -yearmean -mergetime '+inpath+folder+'/'+name+'*.nc',\n",
    "                           output = outputfile)\n",
    "                #gmst monthly\n",
    "                outputfile = outpath+'gmst_monthly/gmst_monthly_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                if not os.path.isfile(outputfile):\n",
    "                    cdo.fldmean(input='-chname,tas,gmst_monthly -selvar,tas -monmean -mergetime '+inpath+folder+'/'+name+'*.nc',\n",
    "                           output = outputfile)\n",
    "                # gmst arctic\n",
    "                outputfile = outpath+'gmst_arctic/gmst_arctic_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                if not os.path.isfile(outputfile):\n",
    "                    cdo.fldmean(input='-chname,tas,gmst_arctic -sellonlatbox,0,360,66,90 -selvar,tas -monmean -mergetime '+inpath+folder+'/'+name+'*.nc',\n",
    "                           output = outputfile)\n",
    "                    \n",
    "            elif variable == 'siconc':\n",
    "                outputfile = outpath+'sia_'+hemisphere+'/sia_'+hemisphere+'_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                inputfile = inpath+folder+'/'+name+'*.nc'\n",
    "                if not os.path.isfile(outputfile):\n",
    "                    gridfile = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'.nc'\n",
    "                    if not os.path.isfile(gridfile):\n",
    "                        cdo.gridarea(input = '-sellonlatbox,'+lonlatbox+' -selvar,siconc -mergetime '+inpath+folder+'/'+name+'*.nc',\n",
    "                                output = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'_calc.nc')\n",
    "                        gridfile = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'_calc.nc'\n",
    "                    # SIA\n",
    "                    cdo.fldsum(input = '-chname,siconc,sia -divc,1e14 -mul '+gridfile+' -sellonlatbox,'+lonlatbox+' -setmissval,0 -selname,siconc -mergetime '+inputfile,\n",
    "                                               output = outputfile)\n",
    "                    # SIE\n",
    "                    outputfile = outpath+'sie_'+hemisphere+'/sie_'+hemisphere+'_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                    cdo.fldsum(input = '-chname,siconc,sie -divc,1e14 -mul '+gridfile+' -setrtoc2,15,100,100,0 -sellonlatbox,'+lonlatbox+' -setmissval,0 -selname,siconc -mergetime '+inputfile,\n",
    "                                               output = outputfile)\n",
    "                    \n",
    "                # calculate mean concentration over 2 periods\n",
    "                if scenario in ['ssp126','ssp245','ssp585']:\n",
    "                    # second part of first period\n",
    "                    outputfile = outpath+'conc_'+hemisphere+'/conc_'+hemisphere+'_2015-2018_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                    if not os.path.isfile(outputfile):\n",
    "                        cdo.remapnn(outpath+'conc_grid_'+hemisphere+'.nc',\n",
    "                                    input='-ymonmean -selyear,2015/2018 -selname,siconc -mergetime '+inputfile,\n",
    "                                    output = outputfile)\n",
    "                    # second period\n",
    "                    outputfile = outpath+'conc_'+hemisphere+'/conc_'+hemisphere+'_2045-2054_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                    if not os.path.isfile(outputfile):\n",
    "                        cdo.remapnn(outpath+'conc_grid_'+hemisphere+'.nc',\n",
    "                                input='-ymonmean -selyear,2045/2054 -selname,siconc -mergetime '+inputfile,\n",
    "                                output = outputfile)\n",
    "                elif scenario == 'historical':\n",
    "                    # first part of first period\n",
    "                    outputfile = outpath+'conc_'+hemisphere+'/conc_'+hemisphere+'_2009-2014_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                    if not os.path.isfile(outputfile):\n",
    "                        cdo.remapnn(outpath+'conc_grid_'+hemisphere+'.nc',\n",
    "                                input='-ymonmean -selyear,2009/2014 -selname,siconc -mergetime '+inputfile,\n",
    "                                output = outputfile)\n",
    "                        \n",
    "            elif variable == 'sivol':\n",
    "                outputfile = outpath+'siv_'+hemisphere+'/siv_'+hemisphere+'_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                inputfile = inpath+folder+'/'+name+'*.nc'\n",
    "                if not os.path.isfile(outputfile):\n",
    "                    gridfile = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'.nc'\n",
    "                    if not os.path.isfile(gridfile):\n",
    "                        cdo.gridarea(input = '-sellonlatbox,'+lonlatbox+' -selvar,sivol -mergetime '+inpath+folder+'/'+name+'*.nc',\n",
    "                                output = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'_calc.nc')\n",
    "                        gridfile = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'_calc.nc'\n",
    "                    cdo.fldsum(input = '-chname,sivol,siv -divc,1e12 -mul '+gridfile+' -sellonlatbox,'+lonlatbox+' -selname,sivol -mergetime '+inputfile,\n",
    "                                   output = outputfile)\n",
    "            elif variable == 'sithick':\n",
    "                outputfile = outpath+'siv_'+hemisphere+'/siv_'+hemisphere+'_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                name_conc = name.replace('sithick','siconc')\n",
    "                folder_conc = folder.replace('sithick','siconc')\n",
    "                inputfile = inpath+folder+'/'+name+'*.nc'\n",
    "                conc_input = '-divc,100 -mergetime '+inpath+folder_conc+'/'+name_conc+'*.nc'\n",
    "        \n",
    "                if not os.path.isfile(outputfile):\n",
    "                    gridfile = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'.nc'\n",
    "                    if not os.path.isfile(gridfile):\n",
    "                        cdo.gridarea(input = '-sellonlatbox,'+lonlatbox+' -selvar,sithick -mergetime '+inpath+folder+'/'+name+'*.nc',\n",
    "                                output = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'_calc.nc')\n",
    "                        gridfile = outpath+'gridareas/'+model+'_gridarea_'+hemisphere+'_calc.nc'\n",
    "                    cdo.selname('sithick',input='-mergetime '+inputfile,output='temp_thk.nc')\n",
    "                    try:\n",
    "                        cdo.selname('siconc',input=conc_input,output='temp_conc.nc')\n",
    "                    except:\n",
    "                        print('No concentration data!')\n",
    "                        continue\n",
    "                    cdo.fldsum(input = '-monmean -chname,sithick,siv -divc,1e12 -mul '+gridfile+' -sellonlatbox,'+lonlatbox+' -mul temp_thk.nc temp_conc.nc',\n",
    "                                output=outputfile)\n",
    "                    os.remove('temp_conc.nc')\n",
    "                    os.remove('temp_thk.nc')\n",
    "            elif variable == 'siarea'+hemisphere[0]:\n",
    "                outputfile = outpath+'sia_'+hemisphere+'/sia_'+hemisphere+'_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                if not os.path.isfile(outputfile):\n",
    "                    cdo.monmean(input='-chname,siarea'+hemisphere[0]+',sia -mergetime '+inpath+folder+'/'+name+'*.nc',output=outputfile)\n",
    "            elif variable == 'sivol'+hemisphere[0]:\n",
    "                outputfile = outpath+'siv_'+hemisphere+'/siv_'+hemisphere+'_'+model+'_'+member+'_'+scenario+'.nc'\n",
    "                if not os.path.isfile(outputfile):\n",
    "                    cdo.monmean(input='-chname,sivol'+hemisphere[0]+',siv -mergetime '+inpath+folder+'/'+name+'*.nc',output=outputfile)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('*************** ERROR ***************')\n",
    "            print(model, variable, scenario, member)\n",
    "            print('*************************************')\n",
    "            \n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data from all nc files and put into mat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This script reads data from the nc files of every simulation and every variable\n",
    "# and writes it to a .mat file, for every scenario separately\n",
    "# The script works with CMIP5 and CMIP6 data\n",
    "# Author: Jakob Doerr (jakob.doerr@mpimet.mpg.de)\n",
    "# Date: 25.09.2019\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from netCDF4 import Dataset,num2date\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from itertools import cycle\n",
    "from scipy.io import savemat,loadmat\n",
    "from dateutil.relativedelta import *\n",
    "from IPython.display import display,clear_output\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "%matplotlib inline\n",
    "\n",
    "# ------------- Define functions -------------------------------------------------------------\n",
    "# this function reads an nc file and extracts data \n",
    "def read_in(filename,month,var):\n",
    "    file = Dataset(filename)\n",
    "    try:\n",
    "        time = np.squeeze(num2date(file.variables['time'][:].copy(),file.variables[\"time\"].units,calendar = file.variables[\"time\"].calendar))\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        time = np.squeeze(file.variables['time'][:].copy())\n",
    "        time = [datetime.datetime(years[scenario][0],1,1)+relativedelta(months=+val) for val in time]\n",
    "\n",
    "    data = np.squeeze(file.variables[var][:].copy())\n",
    "    if 'HadGEM2-ES' in filename or 'HadGEM2-CC' in filename: # some cmip5 models...\n",
    "        time = time[1::]\n",
    "        data = data[1::]\n",
    "    raw_years = np.asarray([time[i].year for i in range(len(time))])\n",
    "    if month >= 0:\n",
    "        model_months = np.asarray([time[i].month for i in range(len(time))])\n",
    "        raw_years = raw_years[np.where(model_months==month+1)]\n",
    "        data = data[np.where(model_months==month+1)]\n",
    "        #time_hist = time_hist[month::12]\n",
    "        #data_hist = data_hist[month::12]\n",
    "    return raw_years,data\n",
    "\n",
    "# this function reads the data and sorts it by month (if necessary) and sets NaNs where data is missing\n",
    "# if the scenario is SSP XXX, the function will still try to read historical data, and the final npy\n",
    "# file of the scenario will still contain historical data. This makes it easier to look at long timeseries of\n",
    "# historical and future scenarios, as they are then already matched.\n",
    "def get_variable(month=-1,filename='',var='',scenario=''):\n",
    "    try:\n",
    "        try:\n",
    "            time_hist,data_hist = read_in(filename+'_historical.nc',month,var)\n",
    "        except Exception as e: # no data for historical? -> fill with nans\n",
    "            #print(e)\n",
    "            #print(filename)\n",
    "            time_hist = years['historical']\n",
    "            data_hist = np.zeros(len(years['historical'])) * np.nan\n",
    "        if scenario != 'historical': # concatenate historical and ssp (rcp)\n",
    "            time_future,data_future = read_in(filename+'_'+scenario+'.nc',month,var)\n",
    "            # avoid double times (yes, some models have that...)\n",
    "            data_hist = [data_hist[i] for i in range(len(data_hist)) if time_hist[i] < time_future[0]]\n",
    "            time_hist = [time_hist[i] for i in range(len(time_hist)) if time_hist[i] < time_future[0]]\n",
    "\n",
    "            time = np.concatenate([time_hist,time_future])\n",
    "            data = np.concatenate([data_hist,data_future])\n",
    "        else: # only historical\n",
    "            time = time_hist\n",
    "            data = data_hist\n",
    "\n",
    "        new_time = years[scenario]\n",
    "        out_data = np.zeros(len(new_time))*np.nan\n",
    "        out_data[np.isin(new_time,time)] = data[np.isin(time,new_time)]\n",
    "\n",
    "        # some special cases...\n",
    "        if var in ['sia','sie'] and np.nanmean(out_data) > 1000:\n",
    "            print(filename)\n",
    "            out_data /= 1e12\n",
    "        # correct models with wrong units in volume \n",
    "        if var in ['siv']:\n",
    "            order = np.floor(np.log10(np.nanmean(out_data)))\n",
    "            if order > 9:\n",
    "                out_data/= 1e12\n",
    "            elif order > 2:\n",
    "                out_data/= 1e3\n",
    "        if len(data.shape)> 1:\n",
    "            raise ValueError('Data array too big!')\n",
    "\n",
    "    except Exception as e:\n",
    "        out_data = np.zeros(len(years[scenario]))*np.nan\n",
    "    return out_data\n",
    "\n",
    "# ---------------- Start ------------------------------------------------------\n",
    "\n",
    "for cmipname in ['CMIP5','CMIP6']:\n",
    "    if cmipname == 'CMIP6':\n",
    "        scenarios = ['historical','ssp119','ssp126','ssp245','ssp585']\n",
    "        histend = 2014\n",
    "    else:\n",
    "        scenarios = ['historical','rcp26','rcp45','rcp85']\n",
    "        histend = 2005\n",
    "\n",
    "\n",
    "    years = {'historical':np.arange(1850,histend+1),'ssp119':np.arange(1850,2100),'ssp126':np.arange(1850,2100),\n",
    "             'ssp245':np.arange(1850,2100),'ssp585':np.arange(1850,2100),'rcp26':np.arange(1850,2100),\n",
    "            'rcp85':np.arange(1850,2100),'rcp45':np.arange(1850,2100),'1pctCO2':np.arange(1850,2000)}\n",
    "\n",
    "    basepath = '/work/mh0033/m300681/IPCC/processed_data/'+cmipname+'/'\n",
    "    hemisphere = 'nh'\n",
    "\n",
    "    \n",
    "\n",
    "    # before we start, create a list of all files for the three variables sia, siv, gmst\n",
    "    # and save it to txt files\n",
    "    for var in ['gmst','sia_'+hemisphere,'siv_'+hemisphere]:\n",
    "        files_list = list(sorted([os.path.basename(x) for x in glob.glob(basepath+var+'/'+var+'_*.nc')]))\n",
    "        with open('/home/mpim/m300681/SIMIP/'+var+'_'+cmipname+'_files.txt', 'w') as f:\n",
    "            for item in files_list:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "            f.close()\n",
    "    # --------------------------------------------------------------------------\n",
    "    # start\n",
    "    for scenario in scenarios:\n",
    "\n",
    "        print('************************'+ scenario+ '******************************')\n",
    "        len_years = len(years[scenario])\n",
    "\n",
    "        # create list of available models that have GMST or SIA\n",
    "        models = {}\n",
    "        models['sia'] = list(sorted(set([os.path.basename(x).split('_')[2] \n",
    "                                      for x in glob.glob(basepath+'sia_'+hemisphere+'/'+'sia_'+hemisphere+'_*_'+scenario+'.nc')])))\n",
    "        models['gmst'] = list(sorted(set([os.path.basename(x).split('_')[1] \n",
    "                                      for x in glob.glob(basepath+'gmst/gmst_*_'+scenario+'.nc')])))\n",
    "        models['siv'] = list(sorted(set([os.path.basename(x).split('_')[2] \n",
    "                                      for x in glob.glob(basepath+'siv_'+hemisphere+'/'+'siv_'+hemisphere+'_*_'+scenario+'.nc')])))\n",
    "\n",
    "        modellist = list(set(np.concatenate([models['sia'],models['gmst'],models['siv']])))\n",
    "        modellist = list(sorted(set(modellist)))\n",
    "\n",
    "        print(modellist)\n",
    "        # construct a list of simulations for every model in the file\n",
    "        # I will just fill the simulations of the other variables with NaNs if they don't have any \n",
    "        # data\n",
    "        simulations = []\n",
    "        for model in modellist:\n",
    "            all_members = []\n",
    "            for var in ['gmst','sia_'+hemisphere,'siv_'+hemisphere]:\n",
    "                members = [os.path.basename(x) for x in glob.glob(basepath+var+'/'+var+'_'+model+'_*_'+scenario+'.nc')]\n",
    "                members  = [members[i][len(model)+len(var)+2:-(len(scenario)+4)] for i in range(len(members))]\n",
    "                for member in members:\n",
    "                    all_members.append(member)\n",
    "            ensemblemembers = list(sorted(set(all_members),key = lambda x: int(x[1:3].replace('i',''))))\n",
    "            for member in ensemblemembers:\n",
    "                simulations.append(model+'_'+member)\n",
    "\n",
    "        print(simulations)\n",
    "        print(len(simulations))\n",
    "\n",
    "        data_array = {}\n",
    "        ### GMST ###### \n",
    "        # Separate because it's the only one where we only need annual mean\n",
    "        print('GMST')\n",
    "        var = 'gmst'\n",
    "        data_array['gmst'] = np.zeros((len(simulations),len_years))\n",
    "        for i,simulation in enumerate(simulations):\n",
    "            filename = basepath+var+'/'+var+'_'+simulation\n",
    "            try:\n",
    "                data = get_variable(-1,filename,var,scenario)\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                #print(simulation)\n",
    "                data = np.zeros(len_years)*np.nan\n",
    "            data_array['gmst'][i,:] = data\n",
    "\n",
    "        ### SIA, SIV #####\n",
    "        pool = mp.Pool(processes=12) # parallel calculation\n",
    "        for var in ['sia','siv']:\n",
    "            print(var)\n",
    "            data_array[var+'_'+hemisphere] = np.zeros((len(simulations),len_years,12))\n",
    "            count = 0\n",
    "            for i,simulation in enumerate(simulations):\n",
    "                # print percentage\n",
    "\n",
    "                print('\\r',str(np.round(100*count/(len(simulations)),1))+'%',end='')\n",
    "                filename = basepath+var+'_'+hemisphere+'/'+var+'_'+hemisphere+'_'+simulation\n",
    "\n",
    "                data = pool.map(partial(get_variable,filename=filename,\n",
    "                                var=var,scenario=scenario),range(12))\n",
    "\n",
    "                data = np.asarray(data)\n",
    "                data = np.swapaxes(data,0,1)\n",
    "                # more special cases...\n",
    "                if var == 'siv': # remove high sea-ice volume models\n",
    "                    data[data > 70] = np.nan\n",
    "                    if hemisphere == 'nh' and cmipname == 'CMIP5' and 'GISS' in simulation:\n",
    "                        data *= np.nan\n",
    "\n",
    "                data_array[var+'_'+hemisphere][i,:,:] = data\n",
    "                count += 1\n",
    "            print()       \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print('Pool joined.')\n",
    "        # save to mat file\n",
    "        data_array['Simulation_names'] = simulations\n",
    "        data_array['Years'] = years[scenario]\n",
    "        savemat(cmipname+'_'+scenario+'_'+hemisphere+'.mat',data_array)\n",
    "\n",
    "        # additionally, save the individual modellists (for every variable) to files\n",
    "        # do this only for the NH, because that is what we look at in the paper\n",
    "        if hemisphere == 'nh':\n",
    "            for variable in ['sia','gmst','siv']:\n",
    "                if variable == 'gmst':\n",
    "                    var = variable\n",
    "                else:\n",
    "                    var = variable+'_nh'\n",
    "                f = open('/home/mpim/m300681/IPCC/CMIP6_models/'+scenario+'_'+variable+'_available_models.txt','w')\n",
    "                f.write('Model,N_ens\\n')\n",
    "                for model in models[variable]:\n",
    "                    n_ens = len([os.path.basename(x) for x in glob.glob(basepath+var+'/'+var+'_'+model+'_*_'+scenario+'.nc')])\n",
    "                    f.write(model+','+str(n_ens)+'\\n')\n",
    "                f.close()\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sensitivities for Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this script calculates sensitivity metrics, as well as other metrics such as\n",
    "# the mean sea-ice area and volume over a control period. The calculated metrics are:\n",
    "# sia vs co2; sia vs gmst; gmst vs co2; mean sept. and march sia over a control period;\n",
    "# mean sept. and march siv over a control period; variability of the sept. sia in the\n",
    "# piControl run.\n",
    "# For SIV and the variability of the piControl run, there is no data for the observations, hence\n",
    "# these metrics are set to NaN here.\n",
    "# Author: Jakob Doerr (jakob.doerr@mpimet.mpg.de)\n",
    "# Date: 25.09.2019\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset,num2date\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "\n",
    "hemisphere = 'nh'\n",
    "co2path = '/work/mh0033/m300681/IPCC/processed_data/emissions/'\n",
    "obspath = '/work/mh0033/m300681/IPCC/processed_data/obs/'\n",
    "analysis_period = np.arange(1979,2015) \n",
    "control_period = np.arange(1979,1999) # control period within the analysis period\n",
    "last_period = np.arange(2005,2015) # last period\n",
    "# ----------------------------------------------------------\n",
    "# observations of sia \n",
    "sia_names = ['nsidc_nt','nsidc_bt','osisaf']\n",
    "data = np.load(obspath+'SIA_observations_'+hemisphere+'.npy',allow_pickle=True)\n",
    "obs_sia = {}\n",
    "for obs_name in sia_names:\n",
    "    obs_sia[obs_name] = {}\n",
    "    obs_data = data[obs_name]\n",
    "    obs_time = data['years']\n",
    "    for month in np.arange(1,13):\n",
    "        obs_sia[obs_name][month] = obs_data[\"{:02d}\".format(month)][np.isin(obs_time,analysis_period)]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# observations of GMST\n",
    "gmst_names = ['noaa','gistemp','HadCRUT','berkeley']\n",
    "obs_gmst = {}\n",
    "for obs_name in gmst_names:\n",
    "\n",
    "    gmstfile = Dataset('/work/mh0033/m300681/IPCC/processed_data/obs/GMST_'+obs_name+'_yearly.nc')\n",
    "    obs_data = np.squeeze(gmstfile.variables['tas_ano'][:].data.copy()) \n",
    "    time = np.squeeze(num2date(gmstfile.variables['time'][:].copy(),\n",
    "                               gmstfile.variables[\"time\"].units,\n",
    "                               calendar = gmstfile.variables[\"time\"].calendar))\n",
    "    obs_time = np.asarray([time[i].year for i in range(len(time))])\n",
    "\n",
    "    obs_gmst[obs_name] = np.zeros(len(analysis_period))*np.nan\n",
    "    # map the data to the correct years\n",
    "    obs_gmst[obs_name][np.isin(analysis_period,obs_time)] = obs_data[np.isin(obs_time,analysis_period)]    \n",
    "# ----------------------------------------------------------                    \n",
    "# co2 data\n",
    "co2 = {}\n",
    "co2_data = np.load(co2path+'CO2_CMIP6.npy',allow_pickle=True)\n",
    "co2data = co2_data['historical'][1,:]\n",
    "co2years = co2_data['historical'][0,:]\n",
    "## co2 for analysis period \n",
    "obs_co2 = co2data[np.isin(co2years,analysis_period)]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# calculate the observed metrics:\n",
    "indic_obs_dict = {} # initialize dictionary\n",
    "\n",
    "ctrl_idx = np.where(np.isin(analysis_period,control_period))\n",
    "last_idx = np.where(np.isin(analysis_period,last_period))\n",
    "\n",
    "# SIA-only metrics:\n",
    "indic_obs_dict['sia_co2'] = np.asarray([stats.linregress(obs_co2,obs_sia[name][9])[0] for name in sia_names])\n",
    "indic_obs_dict['sia_co2']*=1e3\n",
    "indic_obs_dict['sia_mar_co2'] = np.asarray([stats.linregress(obs_co2,obs_sia[name][3])[0] for name in sia_names])\n",
    "indic_obs_dict['sia_mar_co2']*=1e3\n",
    "indic_obs_dict['sia_mean_mar'] = np.asarray([np.nanmean(obs_sia[name][3]) for name in sia_names])\n",
    "indic_obs_dict['sia_mean_sept'] = np.asarray([np.nanmean(obs_sia[name][9]) for name in sia_names])\n",
    "indic_obs_dict['sia_mean_sept_start'] = np.asarray([np.nanmean(obs_sia[name][9][ctrl_idx]) for name in sia_names])\n",
    "indic_obs_dict['sia_mean_sept_end'] = np.asarray([np.nanmean(obs_sia[name][9][last_idx]) for name in sia_names])\n",
    "indic_obs_dict['sia_mean_mar_start'] = np.asarray([np.nanmean(obs_sia[name][3][ctrl_idx]) for name in sia_names])\n",
    "indic_obs_dict['sia_mean_mar_end'] = np.asarray([np.nanmean(obs_sia[name][3][last_idx]) for name in sia_names])\n",
    "max_sia = [np.nanmean(np.max(np.asarray([obs_sia[name][i][last_idx] for i in range(1,13)]),0)) for name in sia_names]\n",
    "min_sia = [np.nanmean(np.min(np.asarray([obs_sia[name][i][last_idx] for i in range(1,13)]),0)) for name in sia_names]\n",
    "indic_obs_dict['sia_annual_cycle'] = np.asarray(max_sia) - np.asarray(min_sia)\n",
    "\n",
    "# GMST-only metrics:\n",
    "indic_obs_dict['gmst_co2'] = np.asarray([stats.linregress(obs_co2,obs_gmst[name])[0] for name in gmst_names])\n",
    "indic_obs_dict['gmst_co2']*=1e3\n",
    "\n",
    "# GMST and SIA metrics:\n",
    "indic_obs_dict['sia_gmst']= np.asarray([stats.linregress(obs_gmst[gmst_name],obs_sia[sia_name][9])[0]\n",
    "                                        for gmst_name in gmst_names for sia_name in sia_names])\n",
    "\n",
    "with open('obs_sensitivity_'+hemisphere+'.npy', 'wb') as file:\n",
    "    pickle.dump(indic_obs_dict,file)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sensitiviy metrics for CMIP5 and CMIP6, create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this script calculates sensitivity metrics, as well as other metrics such as\n",
    "# the mean sea-ice area and volume over a control period. The calculated metrics are:\n",
    "# sia vs co2; sia vs gmst; gmst vs co2; mean sept. and march sia over a control period;\n",
    "# mean sept. and march siv over a control period; variability of the sept. sia in the\n",
    "# piControl run.\n",
    "# For each of the metric except the variability in the piControl run, the script also \n",
    "# calculates an estimate of the internal variability of the metric for those models\n",
    "# with 5 or more members.\n",
    "# Additionally, it writes a LaTex table of the first ice-free year of every model and \n",
    "# every SSP scenario\n",
    "# Author: Jakob Doerr (jakob.doerr@mpimet.mpg.de)\n",
    "# Date: 25.09.2019\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset,num2date\n",
    "import scipy.stats as stats\n",
    "from scipy.io import savemat,loadmat,whosmat\n",
    "import pickle \n",
    "import pandas as pd \n",
    "import glob \n",
    "from math import gamma\n",
    "\n",
    "# define a function that returns the unbiased standard deviation. This is used by all the cells below!\n",
    "def st_dev(data,axis = 0):\n",
    "    data = np.asarray(data[:])\n",
    "    n = len(data[~np.isnan(data)])\n",
    "    \n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    if n > 40:\n",
    "        return np.nanstd(data,axis=axis,ddof=1)\n",
    "    else:\n",
    "        c4 = np.sqrt(2/(n-1))* gamma(n/2) / gamma((n-1)/2)\n",
    "        return np.nanstd(data,axis=axis,ddof=1) / c4\n",
    "\n",
    "co2path = '/work/mh0033/m300681/IPCC/processed_data/emissions/'\n",
    "hemisphere = 'nh'\n",
    "# define titles and units for the table\n",
    "\n",
    "titles = {'sia_co2':'dSIA/dCO2','sia_gmst':'dSIA/dGMST','gmst_co2':'dGMST/dCO2',\n",
    "          'sia_mean_mar':'SIA Mar. (1979-2014)', 'sia_mean_sept':'SIA Sept. (1979-2014)',\n",
    "          'siv_mean_mar':'SIV Mar. (1979-2014)','siv_mean_sept':'SIV Sept. (1979-2014)',\n",
    "          'siv_mean_mar_start':'SIV Mar.','siv_mean_sept_start':'SIV Sept.',\n",
    "          'piCtrl_var':'Sept. SIA variability','ECS':'Equilibrium climate sensitivity [째C]',\n",
    "          'sia_mean_mar_start':'SIA Mar.','sia_mean_sept_start':'SIA Sept.',\n",
    "          'sia_mean_mar_end':'SIA Mar. (2005-2014)','sia_mean_sept_end':'SIA Sept. (2005-2014)'}\n",
    "units = {'sia_co2':'[m$^2$/t]','sia_gmst':'[million km$^2$/째C]','gmst_co2':'[째C/1000Gt]',\n",
    "          'sia_mean_mar':'[million km$^2$]', 'sia_mean_sept':'[million km$^2$]',\n",
    "          'siv_mean_mar':'[thousand km$^3$]','siv_mean_sept':'[thousand km$^3$]',\n",
    "          'siv_mean_mar_start':'[thousand km$^3$]','siv_mean_sept_start':'[thousand km$^3$]',\n",
    "          'piCtrl_var':'[million km$^2$]','ECS':'[째C]',\n",
    "          'sia_mean_mar_start':'[million km$^2$]','sia_mean_sept_start':'[million km$^2$]',\n",
    "          'sia_mean_mar_end':'[million km$^2$]','sia_mean_sept_end':'[million km$^2$]'}\n",
    "\n",
    "\n",
    "for cmip in ['CMIP5','CMIP6']:\n",
    "    cmippath = '/work/mh0033/m300681/IPCC/processed_data/'+cmip+'/'\n",
    "    if cmip == 'CMIP6':\n",
    "        scenario = 'historical' # which scenario is used for the trend analysis\n",
    "    else:\n",
    "        scenario = 'rcp45' # which scenario is used for the trend analysis\n",
    "\n",
    "    analysis_period = np.arange(1979,2015) # period over which to calculate the metrics\n",
    "    control_period = np.arange(1979,1999) # control period over which to calculate the mean values\n",
    "    last_period = np.arange(2005,2015) # last 10 years of analysis period\n",
    "    # -------------------------------------------------------------------\n",
    "    # read in the .mat file with the model data\n",
    "    data = loadmat(cmip+'_'+scenario+'_'+hemisphere+'.mat')\n",
    "    simulations = data['Simulation_names']\n",
    "    modellist = list(sorted(set([i[0:i.find('_')] for i in simulations]), key=lambda s: s.lower()))\n",
    "    print(modellist)\n",
    "    years = data['Years'][0]\n",
    "    # ---------------------------------------------------------------------\n",
    "    # read the CO2 data\n",
    "    co2 = {}\n",
    "    co2_data = np.load(co2path+'CO2_'+cmip+'.npy',allow_pickle=True)\n",
    "    co2data = co2_data[scenario][1,:]\n",
    "    co2years = co2_data[scenario][0,:]\n",
    "    ## co2 for analysis period \n",
    "    hist_co2 = co2data[np.isin(co2years,analysis_period)]\n",
    "    #-----------------------------------------------------------------------\n",
    "    #### create table of sea ice sensitivity, save all indices in a dictionary\n",
    "    # all metrics\n",
    "    metrics = ['sia_gmst','sia_co2','gmst_co2','sia_mean_mar','sia_mean_sept','siv_mean_mar_start','siv_mean_sept_start','siv_mean_sept','siv_mean_mar',\n",
    "               'sia_mean_mar_start','sia_mean_sept_start','sia_mean_mar_end','sia_mean_sept_end','sia_mean_obs','piCtrl_var','ECS',\n",
    "              'sia_mar_co2','sia_annual_cycle']\n",
    "    indic_dict = {}\n",
    "    indic_dict['models'] = modellist\n",
    "    for metric in metrics:\n",
    "        indic_dict[metric] = np.zeros(len(modellist))\n",
    "        indic_dict[metric+'_vari'] = np.zeros(len(modellist))*np.nan\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # calculate the piControl variability. We do this from the original nc files because\n",
    "    # each model has a different period and period length for the piControl run.\n",
    "    months = {3:'March',9:'September'}\n",
    "    for month in [3,9]:\n",
    "        for i,model in enumerate(modellist):\n",
    "            try:\n",
    "                file_name = glob.glob(cmippath+'sia_'+hemisphere+'/sia_'+hemisphere+'_'+model+'_r1i1p*_piControl.nc')[0]\n",
    "                file = Dataset(file_name)\n",
    "                time = np.squeeze(num2date(file.variables['time'][:].copy(),file.variables[\"time\"].units,calendar = file.variables[\"time\"].calendar))  \n",
    "                time = np.asarray([time[i].month for i in range(len(time))])\n",
    "                curr_data = np.squeeze(file.variables['sia'][:].copy())\n",
    "                curr_data = curr_data[np.where(time==month)]\n",
    "                indic_dict['piCtrl_var'][i] = st_dev(curr_data)\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                indic_dict['piCtrl_var'][i] = np.nan\n",
    "            if indic_dict['piCtrl_var'][i] == 0:\n",
    "                indic_dict['piCtrl_var'][i] = np.nan\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate the other metrics from the data of the .mat file\n",
    "    metrics_less = ['sia_gmst','sia_co2','gmst_co2','sia_mean_mar','sia_mean_sept','siv_mean_mar_start',\n",
    "                    'siv_mean_sept_start','sia_mean_mar_start','sia_mean_sept_start','sia_mean_mar_end',\n",
    "                    'sia_mean_sept_end','sia_mar_co2','siv_mean_mar','siv_mean_sept','sia_annual_cycle']\n",
    "    metrics_table = ['sia_gmst','sia_co2','gmst_co2','sia_mean_mar_start','sia_mean_sept_start','siv_mean_mar_start','siv_mean_sept_start','piCtrl_var']\n",
    "\n",
    "    # add variable space for the variability of the metric for each larger ensemble\n",
    "    for metric in metrics:\n",
    "        indic_dict[metric+'_allmember'] = {}\n",
    "\n",
    "    year_idx = np.where(np.isin(years,analysis_period))\n",
    "    ctrl_idx = np.where(np.isin(years,control_period))\n",
    "    last_idx = np.where(np.isin(years,last_period))\n",
    "    # additionally, create a table with all the metrics for all models:\n",
    "    table_list = np.zeros((len(modellist)+4,len(metrics_table)+1)).astype(str) # + 3 because of header, and observations, and ens_mean\n",
    "    table_titles = [titles[metric] for metric in metrics_table]\n",
    "    table_units = [units[metric] for metric in metrics_table]\n",
    "    table_list[0,:] = np.concatenate([['Model'],table_titles])\n",
    "    table_list[1,:] = np.concatenate([[''],table_units])\n",
    "\n",
    "    for i,model in enumerate(modellist): # loop over models\n",
    "        index = [j for j in range(len(simulations)) if model+'_' in simulations[j]] # find the simulations of the model\n",
    "        # initialize a dictionary with the metrics from all simulations of the model\n",
    "        curr_indic = {}\n",
    "\n",
    "        for metric in metrics_less:\n",
    "            curr_indic[metric] = np.zeros(len(index))\n",
    "\n",
    "        # calculate the metrics for each member of the model\n",
    "        for k in range(len(index)):\n",
    "            curr_indic['sia_gmst'][k],_,_,_,_= stats.linregress(np.squeeze(data['gmst'][index[k],year_idx]),\n",
    "                                                               np.squeeze(data['sia_'+hemisphere][index[k],year_idx,8]))\n",
    "\n",
    "            curr_indic['sia_co2'][k],_,_,_,_ = stats.linregress(hist_co2,data['sia_'+hemisphere][index[k],year_idx,8])\n",
    "            curr_indic['sia_co2'][k]*= 1e3\n",
    "\n",
    "            curr_indic['sia_mar_co2'][k],_,_,_,_ = stats.linregress(hist_co2,data['sia_'+hemisphere][index[k],year_idx,2])\n",
    "            curr_indic['sia_mar_co2'][k]*= 1e3\n",
    "\n",
    "            curr_indic['gmst_co2'][k],_,_,_,_ = stats.linregress(hist_co2,data['gmst'][index[k],year_idx])\n",
    "            curr_indic['gmst_co2'][k]*=1e3\n",
    "\n",
    "            curr_indic['sia_mean_mar_start'][k] = np.mean(data['sia_'+hemisphere][index[k],ctrl_idx,2])\n",
    "            curr_indic['sia_mean_sept_start'][k] = np.mean(data['sia_'+hemisphere][index[k],ctrl_idx,8])\n",
    "            curr_indic['sia_mean_mar_end'][k] = np.mean(data['sia_'+hemisphere][index[k],last_idx,2])\n",
    "            curr_indic['sia_mean_sept_end'][k] = np.mean(data['sia_'+hemisphere][index[k],last_idx,8])\n",
    "            curr_indic['sia_mean_mar'][k] = np.mean(data['sia_'+hemisphere][index[k],year_idx,2])\n",
    "            curr_indic['sia_mean_sept'][k] = np.mean(data['sia_'+hemisphere][index[k],year_idx,8])\n",
    "\n",
    "            curr_indic['sia_annual_cycle'][k] = np.mean(np.max(data['sia_'+hemisphere][index[k],last_idx,:],2)-np.min(data['sia_'+hemisphere][index[k],last_idx,:],2))\n",
    "\n",
    "            curr_indic['siv_mean_mar_start'][k] = np.mean(data['siv_'+hemisphere][index[k],ctrl_idx,2])\n",
    "            curr_indic['siv_mean_sept_start'][k] = np.mean(data['siv_'+hemisphere][index[k],ctrl_idx,8])\n",
    "            curr_indic['siv_mean_mar'][k] = np.mean(data['siv_'+hemisphere][index[k],year_idx,2])\n",
    "            curr_indic['siv_mean_sept'][k] = np.mean(data['siv_'+hemisphere][index[k],year_idx,8])\n",
    "\n",
    "        try:\n",
    "            indic_dict['ECS'][i] = ecs[cmip][model]\n",
    "        except:\n",
    "            indic_dict['ECS'][i] = np.nan\n",
    "        # finally, choose the 1st ensemble member as the value for the model for each metric,\n",
    "        # but the single values of the individual ensemble members\n",
    "        for m,metric in enumerate(metrics_less):\n",
    "            indic_dict[metric][i] = curr_indic[metric][0]\n",
    "            indic_dict[metric+'_allmember'][model] = curr_indic[metric]\n",
    "\n",
    "        # write the metric which are written on the table\n",
    "        # model + number of members that have SIA\n",
    "        n_SIA = len(curr_indic['sia_mean_sept'][~np.isnan(curr_indic['sia_mean_sept'])])\n",
    "        table_list[i+2,0] = model.upper() + ' ('+str(n_SIA)+')'\n",
    "        for m,metric in enumerate(metrics_table):\n",
    "            if metric == 'piCtrl_var':   \n",
    "                # put the piCtrl_var in the table_list, at the last place\n",
    "                table_list[i+2,-1] = np.round(indic_dict[metric][i],2)\n",
    "            else:\n",
    "                if np.nanstd(curr_indic[metric]) == 0:\n",
    "                    table_list[i+2,m+1] = np.round(np.nanmean(curr_indic[metric]),2)\n",
    "                else:\n",
    "                    table_list[i+2,m+1] = str(np.round(np.nanmean(curr_indic[metric]),2)) + '+-' + str(np.round(st_dev(curr_indic[metric]),2))\n",
    "\n",
    "        # for models with 4 or more members: estimate the internal variability of the metric over the ensemble dimension\n",
    "        if len(index) >= 3:\n",
    "            for metric in metrics_less:\n",
    "                if len(np.where(~np.isnan(curr_indic[metric]))[0]) >= 3:\n",
    "                    indic_dict[metric+'_vari'][i] = st_dev(curr_indic[metric])\n",
    "\n",
    "    # save to npy file\n",
    "    with open(cmip+'_sensitivity_'+hemisphere+'.npy', 'wb') as file:\n",
    "        pickle.dump(indic_dict,file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    # Do the table\n",
    "\n",
    "    # add multi-model mean:\n",
    "    table_list[-2,0] = 'Multi-model mean'\n",
    "    for m,metric in enumerate(metrics_table):\n",
    "        if metric == 'piCtrl_var':\n",
    "            val_mean = np.nanmean(indic_dict[metric])\n",
    "            table_list[-2,m+1] = str(np.round(val_mean,2))\n",
    "        else:\n",
    "            val_mean = np.nanmean([np.nanmean(indic_dict[metric+'_allmember'][model][:]) for model in modellist])\n",
    "            val_std = st_dev([np.nanmean(indic_dict[metric+'_allmember'][model][:]) for model in modellist])\n",
    "            table_list[-2,m+1] = str(np.round(val_mean,2)) + '+-' + str(np.round(val_std,2))\n",
    "\n",
    "\n",
    "    # add  observations:\n",
    "    obsdata = np.load('obs_sensitivity_'+hemisphere+'.npy',allow_pickle=True)\n",
    "    table_list[-1,0] = 'Observations'\n",
    "    for m,metric in enumerate(metrics_table):\n",
    "        if metric in ['piCtrl_var','siv_mean_mar_start','siv_mean_sept_start']:   \n",
    "            # put the piCtrl_var in the table_list, at the last place\n",
    "            table_list[-1,m+1] = 'nan'\n",
    "        else:\n",
    "            if np.nanstd(obsdata[metric]) == 0:\n",
    "                table_list[-1,m+1] = np.round(np.nanmean(obsdata[metric]),2)\n",
    "            else:\n",
    "                table_list[-1,m+1] = str(np.round(np.nanmean(obsdata[metric]),2)) + '+-' + str(np.round(st_dev(obsdata[metric]),2))\n",
    "\n",
    "    # first, replace nans with '--'\n",
    "    for i in range(len(table_list)):\n",
    "        for j in range(len(table_list[0])):\n",
    "            if 'nan' in table_list[i][j]:\n",
    "                table_list[i][j] = '-'\n",
    "\n",
    "\n",
    "    dd = pd.DataFrame(table_list[1::,1::],index=table_list[1::,0],columns = table_list[0,1::])\n",
    "    print(dd)\n",
    "\n",
    "    with open(cmip+'_sensitivity_'+hemisphere+'.tex','w') as tf:\n",
    "        res = dd.to_latex(column_format='rcccccccc')\n",
    "        res = res.replace('+-','$\\pm$')\n",
    "        res = res.replace('\\\\textasciicircum','^')\n",
    "        res = res.replace('\\\\$','$')\n",
    "        res = res.replace('째','$^{\\circ}$')\n",
    "        tf.write(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sensitiviy metrics for CMIP3, create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this script calculates sensitivity metrics, as well as other metrics such as\n",
    "# the mean sea-ice area and volume over a control period. The calculated metrics are:\n",
    "# sia vs co2; sia vs gmst; gmst vs co2; mean sept. and march sia over a control period;\n",
    "# mean sept. and march siv over a control period; variability of the sept. sia in the\n",
    "# piControl run.\n",
    "# For SIV and the variability of the piControl run, there is no data for CMIP3, hence\n",
    "# these metrics are set to NaN here.\n",
    "# Author: Jakob Doerr (jakob.doerr@mpimet.mpg.de)\n",
    "# Date: 25.09.2019\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import scipy.stats as stats\n",
    "from scipy.io import savemat,loadmat,whosmat\n",
    "import pickle\n",
    "\n",
    "hemisphere = 'nh'\n",
    "co2path = '/work/mh0033/m300681/IPCC/processed_data/emissions/'\n",
    "analysis_period = np.arange(1979,2015) # period over which to calculate the metrics\n",
    "control_period = np.arange(1979,1999) # control period over which to calculate the mean values\n",
    "# function to read cmip3 data from the mat file (mat file was provided externally)\n",
    "def cmip3_data():\n",
    "    data = {}\n",
    "    struct = loadmat('/work/mh0033/m300681/IPCC/processed_data/CMIP3/allData_C3.mat')\n",
    "    model_names = np.concatenate(struct['matched'][0][0][0][0],axis=0)\n",
    "    model_names = [i.replace('_','-') for i in model_names]\n",
    "    models = list(sorted(set(model_names)))\n",
    "    models.remove('iap-fgoals1-0-g')\n",
    "    realizations = struct['matched'][0][0][1][0]\n",
    "    months = struct['matched'][0][0][2][0]\n",
    "    years = np.asarray(sorted(set(struct['matched'][0][0][3][0])))\n",
    "    data['sia_nh'] = struct['matched'][0][0][4]/1e6\n",
    "    data['sia_sh'] = struct['matched'][0][0][5]/1e6\n",
    "    data['sie_nh'] = struct['matched'][0][0][6]/1e6\n",
    "    data['sie_sh'] = struct['matched'][0][0][7]/1e6\n",
    "    data['gmst'] = struct['matched'][0][0][8]\n",
    "    \n",
    "    out_data = {}\n",
    "    out_data['sia_'+hemisphere] = np.zeros((len(model_names),len(years),12))\n",
    "    out_data['gmst'] = np.zeros((len(model_names),len(years)))\n",
    "    for i,model in enumerate(model_names):\n",
    "        out_data['gmst'][i,:] = [np.mean(data['gmst'][i,j*12:j*12+12]) for j in range(len(years))]\n",
    "        for month in np.arange(12):\n",
    "            out_data['sia_'+hemisphere][i,:,month] = data['sia_'+hemisphere][i,month-1::12]\n",
    "    model_names = [i+'_' for i in model_names]\n",
    "    return model_names,models,years,out_data\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# read in data \n",
    "simulations, modellist, years, data = cmip3_data()\n",
    "print(simulations)\n",
    "# ---------------------------------------------------------------------\n",
    "# read the CO2 data. Here, we use CMIP6 co2 data, but it does not\n",
    "# matter, since there is the actual observed emissions until 2017 \n",
    "# in the data\n",
    "co2 = {}\n",
    "co2_data = np.load(co2path+'CO2_CMIP6.npy',allow_pickle=True)\n",
    "co2data = co2_data['historical'][1,:]\n",
    "co2years = co2_data['historical'][0,:]\n",
    "## co2 for analysis period \n",
    "hist_co2 = co2data[np.isin(co2years,analysis_period)]\n",
    "#-----------------------------------------------------------------------\n",
    "#### create table of sea ice sensitivity, save all indices in a dictionary\n",
    "metrics = ['sia_gmst','sia_co2','gmst_co2','sia_mean_mar','sia_mean_sept','siv_mean_mar',\n",
    "                'siv_mean_sept','siv_mean_mar_start','siv_mean_sept_start','sia_mean_mar_start',\n",
    "           'sia_mean_sept_start','sia_mean_mar_end','sia_mean_sept_end','sia_mar_co2','sia_annual_cycle']\n",
    "metrics_table = ['sia_gmst','sia_co2','gmst_co2','sia_mean_mar_start','sia_mean_sept_start']\n",
    "indic_dict = {}\n",
    "indic_dict['models'] = modellist\n",
    "for metric in metrics:\n",
    "    indic_dict[metric] = np.zeros(len(modellist))\n",
    "    indic_dict[metric+'_allmember'] = {}\n",
    "    indic_dict[metric+'_vari'] = np.zeros(len(modellist))*np.nan\n",
    "# add piCtrl_var dummy for CMIP3 (no actual data)\n",
    "indic_dict['piCtrl_var'] = [np.nan]\n",
    "# ------------------------------------------------------------------------------\n",
    "# Calculate the other metrics from the data of the .mat file\n",
    "\n",
    "year_idx = np.where(np.isin(years,analysis_period))\n",
    "ctrl_idx = np.where(np.isin(years,control_period))\n",
    "last_idx = np.where(np.isin(years,last_period))\n",
    "# additionally, create a table with all the metrics for all models:\n",
    "table_list = np.zeros((len(modellist)+4,len(metrics_table)+1)).astype(str)\n",
    "table_titles = [titles[metric] for metric in metrics_table]\n",
    "table_units = [units[metric] for metric in metrics_table]\n",
    "table_list[0,:] = np.concatenate([['Model'],table_titles])\n",
    "table_list[1,:] = np.concatenate([[''],table_units])\n",
    "for i,model in enumerate(modellist): # loop over models\n",
    "    index = [j for j in range(len(simulations)) if model+'_' in simulations[j]] # find the simulations of the model\n",
    "    # initialize a dictionary with the metrics from all simulations of the model\n",
    "    curr_indic = {}\n",
    "    for metric in metrics:\n",
    "        curr_indic[metric] = np.zeros(len(index))\n",
    "    \n",
    "    # calculate the metrics for each member of the model\n",
    "    for k in range(len(index)):\n",
    "        curr_indic['sia_gmst'][k],_,_,_,_= stats.linregress(np.squeeze(data['gmst'][index[k],year_idx]),\n",
    "                                                           np.squeeze(data['sia_'+hemisphere][index[k],year_idx,8]))\n",
    "        \n",
    "        curr_indic['sia_co2'][k],_,_,_,_ = stats.linregress(hist_co2,data['sia_'+hemisphere][index[k],year_idx,8])\n",
    "        curr_indic['sia_co2'][k]*= 1e3\n",
    "        \n",
    "        curr_indic['sia_mar_co2'][k],_,_,_,_ = stats.linregress(hist_co2,data['sia_'+hemisphere][index[k],year_idx,2])\n",
    "        curr_indic['sia_mar_co2'][k]*= 1e3\n",
    "        \n",
    "        curr_indic['gmst_co2'][k],_,_,_,_ = stats.linregress(hist_co2,data['gmst'][index[k],year_idx])\n",
    "        curr_indic['gmst_co2'][k]*=1e3\n",
    "        \n",
    "        curr_indic['sia_mean_mar_start'][k] = np.mean(data['sia_'+hemisphere][index[k],ctrl_idx,2])\n",
    "        curr_indic['sia_mean_sept_start'][k] = np.mean(data['sia_'+hemisphere][index[k],ctrl_idx,8])\n",
    "        curr_indic['sia_mean_mar_end'][k] = np.mean(data['sia_'+hemisphere][index[k],last_idx,2])\n",
    "        curr_indic['sia_mean_sept_end'][k] = np.mean(data['sia_'+hemisphere][index[k],last_idx,8])\n",
    "        curr_indic['sia_mean_mar'][k] = np.mean(data['sia_'+hemisphere][index[k],year_idx,2])\n",
    "        curr_indic['sia_mean_sept'][k] = np.mean(data['sia_'+hemisphere][index[k],year_idx,8])\n",
    "        \n",
    "        curr_indic['sia_annual_cycle'][k] = np.mean(np.max(data['sia_'+hemisphere][index[k],last_idx,:],2)-np.min(data['sia_'+hemisphere][index[k],last_idx,:],2))\n",
    "        # dummies for SIV, no SIV data for cmip3 (until now)\n",
    "        curr_indic['siv_mean_mar'][k] = np.nan\n",
    "        curr_indic['siv_mean_sept'][k] = np.nan\n",
    "        curr_indic['siv_mean_mar_start'][k] = np.nan\n",
    "        curr_indic['siv_mean_sept_start'][k] = np.nan\n",
    "        \n",
    "    # write the table array\n",
    "    # model + number of members that have SIA\n",
    "    n_SIA = len(curr_indic['sia_mean_sept'][~np.isnan(curr_indic['sia_mean_sept'])])\n",
    "    table_list[i+2,0] = model.upper() + ' ('+str(n_SIA)+')'\n",
    "    for m,metric in enumerate(metrics_table):\n",
    "        if np.nanstd(curr_indic[metric]) == 0:\n",
    "            table_list[i+2,m+1] = np.round(np.nanmean(curr_indic[metric]),2)\n",
    "        else:\n",
    "            table_list[i+2,m+1] = str(np.round(np.nanmean(curr_indic[metric]),2)) + '+-' + str(np.round(st_dev(curr_indic[metric]),2))\n",
    "\n",
    "        # put the piCtrl_var in the table_list, at the last place, dummy for CMIP3\n",
    "        #table_list[i+1,-1] = np.round(indic_dict['piCtrl_var'][0],2)\n",
    "    # finally, choose the 1st ensemble member as the value for the model for each metric\n",
    "    for metric in metrics:\n",
    "        indic_dict[metric][i] = curr_indic[metric][0]\n",
    "        indic_dict[metric+'_allmember'][model] = curr_indic[metric]\n",
    "    # for models with 3 or more members: estimate the internal variability of the metric over the ensemble dimension\n",
    "    if len(index) >= 3:\n",
    "        for metric in metrics_table:\n",
    "            if len(np.where(~np.isnan(curr_indic[metric]))[0]) >= 3:\n",
    "                indic_dict[metric+'_vari'][i] = st_dev(curr_indic[metric])    \n",
    "# save to npy file\n",
    "with open('CMIP3_sensitivity_'+hemisphere+'.npy', 'wb') as file:\n",
    "    pickle.dump(indic_dict,file)\n",
    "    file.close()\n",
    "    \n",
    "# Do the table\n",
    "# add multi-model mean:\n",
    "table_list[-2,0] = 'Multi-model mean'\n",
    "for m,metric in enumerate(metrics_table):\n",
    "    if metric == 'piCtrl_var':\n",
    "        val_mean = np.nanmean(indic_dict[metric])\n",
    "        table_list[-2,m+1] = str(np.round(val_mean,2))\n",
    "    else:\n",
    "        val_mean = np.nanmean([np.nanmean(indic_dict[metric+'_allmember'][model][:]) for model in modellist])\n",
    "        val_std = st_dev([np.nanmean(indic_dict[metric+'_allmember'][model][:]) for model in modellist])\n",
    "        table_list[-2,m+1] = str(np.round(val_mean,2)) + '+-' + str(np.round(val_std,2))\n",
    "\n",
    "\n",
    "# add  observations:\n",
    "obsdata = np.load('obs_sensitivity_'+hemisphere+'.npy',allow_pickle=True)\n",
    "table_list[-1,0] = 'Observations'\n",
    "for m,metric in enumerate(metrics_table):\n",
    "    if metric in ['piCtrl_var','siv_mean_mar_start','siv_mean_sept_start']:   \n",
    "        # put the piCtrl_var in the table_list, at the last place\n",
    "        table_list[-1,m+1] = 'nan'\n",
    "    else:\n",
    "        if np.nanstd(obsdata[metric]) == 0:\n",
    "            table_list[-1,m+1] = np.round(np.nanmean(obsdata[metric]),2)\n",
    "        else:\n",
    "            table_list[-1,m+1] = str(np.round(np.nanmean(obsdata[metric]),2)) + '+-' + str(np.round(st_dev(obsdata[metric]),2))\n",
    "\n",
    "\n",
    "# first, replace nans with '--'\n",
    "for i in range(len(table_list)):\n",
    "    for j in range(len(table_list[0])):\n",
    "        if 'nan' in table_list[i][j]:\n",
    "            table_list[i][j] = '-'\n",
    "            \n",
    "\n",
    "dd = pd.DataFrame(table_list[1::,1::],index=table_list[1::,0],columns = table_list[0,1::])\n",
    "print(dd)\n",
    "with open('CMIP3_sensitivity_'+hemisphere+'.tex','w') as tf:\n",
    "    res = dd.to_latex(column_format='rccccc')\n",
    "    res = res.replace('+-','$\\pm$')\n",
    "    res = res.replace('\\\\textasciicircum','^')\n",
    "    res = res.replace('\\\\$','$')\n",
    "    res = res.replace('째','$^{\\circ}$')\n",
    "    tf.write(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constrain ice-free projections based on single member performance for every model, make Ice free table, highlight selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat,loadmat,whosmat\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def getOverlap(a, b):\n",
    "    return a[0] <= b[0] <= a[1] or b[0] <= a[0] <= b[1]\n",
    "\n",
    "data = {} # data array for all the sensitivities\n",
    "cmip = 'CMIP6'\n",
    "hemisphere = 'nh'\n",
    "scenarios = ['ssp119','ssp126','ssp245','ssp585']\n",
    "verbose = False\n",
    "# variability for metrics\n",
    "metric_data = np.load('CMIP6_sensitivity_'+hemisphere+'.npy',allow_pickle=True)\n",
    "# get models that have ssp data\n",
    "modellist = []\n",
    "for scen in scenarios:\n",
    "    simulations = loadmat(cmip+'_'+scen+'_'+hemisphere+'.mat')['Simulation_names']\n",
    "    modell = list([i[0:i.find('_')] for i in simulations])\n",
    "    modellist = np.concatenate([modellist,modell])\n",
    "modellist = list(sorted(set(modellist)))\n",
    "print(modellist)\n",
    "# ---------------------------------------------------------------------\n",
    "# read the CO2 data\n",
    "co2path = '/work/mh0033/m300681/IPCC/processed_data/emissions/'\n",
    "co2 = {}\n",
    "co2_data = np.load(co2path+'CO2_'+cmip+'.npy',allow_pickle=True)\n",
    "co2data = co2_data['historical'][1,:]\n",
    "co2years = co2_data['historical'][0,:]\n",
    "## co2 for analysis period \n",
    "hist_co2 = co2data[np.isin(co2years,np.arange(1979,2015))]\n",
    "for scenario in scenarios:   \n",
    "    co2data = co2_data[scenario][1,:]\n",
    "    co2years = co2_data[scenario][0,:]\n",
    "    co2[scenario] = co2data[np.isin(co2years,np.arange(1850,2100))]\n",
    "    \n",
    "control_years = np.arange(1850,1900)\n",
    "# observed metrics\n",
    "obs_data = np.load('obs_sensitivity_'+hemisphere+'.npy',allow_pickle=True)\n",
    "\n",
    "best_models = []\n",
    "for i,model in enumerate(modellist):\n",
    "    is_good = False # flag whether model is in the selected list\n",
    "    for j in range(len(metric_data['sia_gmst_allmember'][model])):\n",
    "        # check if member is within tolerance of observations\n",
    "        if not is_good:\n",
    "            if verbose:\n",
    "                print(model,j)\n",
    "\n",
    "            for metric in ['sia_co2','sia_mean_sept_end']:\n",
    "                tolerance = 2*np.sqrt(np.nanmean(metric_data[metric+'_vari'])**2+\n",
    "                              st_dev(obs_data[metric])**2)\n",
    "                val = metric_data[metric+'_allmember'][model][j]\n",
    "                obs_metric = np.nanmean(obs_data[metric])\n",
    "                obs_interval = [obs_metric - tolerance,obs_metric + tolerance]\n",
    "\n",
    "                is_good = False\n",
    "                if verbose:\n",
    "                    print(metric)\n",
    "                # check if the member comes close to the observations\n",
    "                if verbose:\n",
    "                    print(obs_metric-tolerance ,val,obs_metric+tolerance)\n",
    "                if min(obs_interval) <= val and val <= max(obs_interval):\n",
    "                    is_good = True\n",
    "\n",
    "                if not is_good:\n",
    "                    break\n",
    "            if is_good:\n",
    "                break\n",
    "    if is_good:\n",
    "        if verbose:\n",
    "            print('SUCCESS!')\n",
    "        best_models.append(model)\n",
    "                \n",
    "best_models = list(sorted(set(best_models)))\n",
    "print('Good models:')     \n",
    "print(best_models)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# #######################################################################################\n",
    "# # analyze for all models the year where the ice disappears in september (<1 million km^2)\n",
    "ordinates = ['temp','time','co2']\n",
    "overload = {'co2':10000,'temp':5,'time':2100}\n",
    "if cmip == 'CMIP6':\n",
    "    scenarios = ['ssp119','ssp126','ssp245','ssp585']\n",
    "elif cmip == 'CMIP5':\n",
    "    scenarios = ['rcp45','rcp85']\n",
    "    \n",
    "scen_years = {'ssp119':np.arange(2015,2100),'ssp126':np.arange(2015,2100),\n",
    "         'ssp245':np.arange(2015,2100),'ssp585':np.arange(2015,2100),\n",
    "        'rcp85':np.arange(2006,2100),'rcp45':np.arange(2006,2100)}\n",
    "\n",
    "control_years = np.arange(1850,1900) # control period for temperature anomalies\n",
    "\n",
    "for ordinate in ordinates:\n",
    "    ice_free_table = {}\n",
    "    ice_free = {}\n",
    "    for scenario in scenarios:\n",
    "        data = loadmat(cmip+'_'+scenario+'_'+hemisphere+'.mat')\n",
    "        years = data['Years'][0]\n",
    "        simulations = data['Simulation_names']\n",
    "        \n",
    "        ice_free_table[scenario] = []\n",
    "        ice_free[scenario] = {}\n",
    "        for m,model in enumerate(modellist):\n",
    "            # find the simulations of the model\n",
    "            index = [j for j in range(len(simulations)) if model+'_' in simulations[j]] \n",
    "            curr_years = []\n",
    "            curr_overload = overload[ordinate]\n",
    "            for i,ind in enumerate(index):#\n",
    "                curr_data = data['sia_'+hemisphere][ind,:,8]\n",
    "                if ~np.isnan(np.mean(curr_data)): # only analyze if there are no NaNs in the data\n",
    "                    try:\n",
    "                        if ordinate == 'time':\n",
    "                            curr_years.append(int(years[curr_data<1.][0]))\n",
    "                        elif ordinate == 'co2':\n",
    "                            curr_years.append(np.round(int(co2[scenario][curr_data<1.][0])))\n",
    "                        elif ordinate == 'temp':\n",
    "                            curr_temp = data['gmst'][ind,:]\n",
    "                            curr_years.append(curr_temp[curr_data<1.][0] - np.nanmean(data['gmst'][ind,np.isin(years,control_years)]))\n",
    "                        curr_overload = overload[ordinate]\n",
    "                    except Exception as e:\n",
    "                        if ordinate == 'temp':\n",
    "                            curr_overload = np.round(np.max(curr_temp) - np.nanmean(data['gmst'][ind,np.isin(years,control_years)]),1)\n",
    "                        elif ordinate == 'co2':\n",
    "                            curr_overload = np.round(np.max(co2[scenario]))\n",
    "                        else:\n",
    "                            curr_overload = 2100\n",
    "                        curr_years.append(curr_overload + 1)\n",
    "                else:\n",
    "                    curr_years.append(np.nan)\n",
    "\n",
    "\n",
    "            # if there was no data for this model, set to NaN\n",
    "            if len(curr_years) == 0:\n",
    "                curr_years = [np.nan]\n",
    "                curr_overload = np.nan\n",
    "\n",
    "            # write all the single members to the data dictionary\n",
    "            ice_free[scenario][model] = curr_years\n",
    "\n",
    "            if ordinate == 'temp':\n",
    "                min_val = np.round(min(curr_years),1)\n",
    "                max_val = np.round(max(curr_years),1)\n",
    "            else:\n",
    "                min_val = min(curr_years)\n",
    "                max_val = max(curr_years)\n",
    "            if min_val > curr_overload:\n",
    "                ice_free_table[scenario].append('>'+str(curr_overload))\n",
    "            elif ordinate == 'time' and max_val < 2014:\n",
    "                ice_free_table[scenario].append('<2014')\n",
    "            elif ordinate == 'time' and min_val < 2014:\n",
    "                ice_free_table[scenario].append('<2014 - '+str(max_val))\n",
    "            elif np.isnan(min_val):\n",
    "                ice_free_table[scenario].append('--')\n",
    "            elif min_val == max_val:\n",
    "                ice_free_table[scenario].append(str(min_val))\n",
    "            else:\n",
    "                if max_val <= overload[ordinate]:\n",
    "                    ice_free_table[scenario].append(str(min_val)+' - '+str(max_val))\n",
    "                else:\n",
    "                    ice_free_table[scenario].append(str(min_val)+' - >'+str(curr_overload))\n",
    "            # append the number of ensemble members\n",
    "            curr_years = np.asarray(curr_years)\n",
    "            nmember = len(curr_years[~np.isnan(curr_years)])\n",
    "            if nmember>0:\n",
    "                ice_free_table[scenario][-1] += ' ('+str(nmember)+')'\n",
    "    # write the results in a table\n",
    "    table_list = []\n",
    "    table_list.append(np.concatenate([[''],[x.upper() for x in scenarios]]))\n",
    "    for i,model in enumerate(modellist):\n",
    "        if model in best_models:\n",
    "            table_list.append(np.concatenate([['\\textbf{'+model+'}'],['\\textbf{'+ice_free_table[scenario][i]+'}' for scenario in scenarios]]))\n",
    "        else:\n",
    "            table_list.append(np.concatenate([[model],[ice_free_table[scenario][i] for scenario in scenarios]]))\n",
    "    # save the table as .tex file\n",
    "    table_list = np.asarray(table_list)\n",
    "    dd = pd.DataFrame(table_list[1::,1::],index=table_list[1::,0],columns = table_list[0,1::])\n",
    "    if ordinate == 'time':\n",
    "        print(dd)\n",
    "    with open('SIMIP_ice_free_'+ordinate+'.tex','w') as tf:\n",
    "        res = dd.to_latex(column_format='ccccc')\n",
    "        res = res.replace('+-','$\\pm$')\n",
    "        res = res.replace('>','$>$')\n",
    "        res = res.replace('<','$<$')\n",
    "        res = res.replace('\\\\textbackslash','\\\\')\n",
    "        res = res.replace('\\{','{')\n",
    "        res = res.replace('\\}','}')\n",
    "        tf.write(res)\n",
    "\n",
    "    # safe the ice free years\n",
    "    ice_free['models'] = modellist\n",
    "    \n",
    "    with open(cmip+'_ice_free_'+ordinate+'.npy','wb') as myFile:\n",
    "        pickle.dump(ice_free,myFile)\n",
    "        myFile.close()\n",
    "        \n",
    "    # selected models\n",
    "    ice_free_select = {}\n",
    "    print('Selected models:')\n",
    "    for scenario in scenarios:\n",
    "        #print(scenario)\n",
    "        ice_free_select[scenario] = []\n",
    "        for i,model in enumerate(best_models):\n",
    "            for val in ice_free[scenario][model]:\n",
    "                ice_free_select[scenario].append(val)\n",
    "        ice_free_select[scenario] = np.asarray(ice_free_select[scenario])\n",
    "        #print(ice_free_select[scenario])\n",
    "        #print(np.nanmin(ice_free_select[scenario]),np.nanmax(ice_free_select[scenario]))\n",
    "    with open(cmip+'_ice_free_select_'+ordinate+'.npy','wb') as myFile:\n",
    "        pickle.dump(ice_free_select,myFile)\n",
    "        myFile.close()\n",
    "        \n",
    "    # all models\n",
    "    ice_free_select = {}\n",
    "    print('All models:')\n",
    "    for scenario in scenarios:\n",
    "        #print(scenario)\n",
    "        ice_free_select[scenario] = []\n",
    "        for i,model in enumerate(modellist):\n",
    "            for val in ice_free[scenario][model]:\n",
    "                ice_free_select[scenario].append(val)\n",
    "        ice_free_select[scenario] = np.asarray(ice_free_select[scenario])\n",
    "        #print(np.nanmin(ice_free_select[scenario]),np.nanmax(ice_free_select[scenario]))\n",
    "        \n",
    "    with open(cmip+'_ice_free_all_'+ordinate+'.npy','wb') as myFile:\n",
    "        pickle.dump(ice_free_select,myFile)\n",
    "        myFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the models that lie within the observations for dSIA/dCO2 and dGMST/dCO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(data['CMIP6']['models'])\n",
    "M1_obs = np.nanmean(obsdata['sia_co2'])\n",
    "print(M1_obs)\n",
    "M1_var = 2*np.sqrt(st_dev(obsdata['sia_co2'])**2+np.nanmean(data['CMIP6']['sia_co2_vari'])**2)\n",
    "M1_int = [M1_obs-M1_var,M1_obs+M1_var]\n",
    "print(M1_int)\n",
    "M2_obs = np.nanmean(obsdata['gmst_co2'])\n",
    "print(M2_obs)\n",
    "M2_var = 2*np.sqrt(st_dev(obsdata['gmst_co2'])**2+np.nanmean(data['CMIP6']['gmst_co2_vari'])**2)\n",
    "M2_int = [M2_obs-M2_var,M2_obs+M2_var]\n",
    "print(M2_int)\n",
    "for i,model in enumerate(data['CMIP6']['models']):\n",
    "    M1_mod = data['CMIP6']['sia_co2'][i]\n",
    "    M2_mod = data['CMIP6']['gmst_co2'][i]\n",
    "    #print()\n",
    "    #print(model)\n",
    "    #print(M2_mod)\n",
    "    #print(M2_int)\n",
    "    #print(M1_mod)\n",
    "    #print(M1_int)\n",
    "    #print()\n",
    "    if min(M1_int) <= M1_mod and M1_mod <= max(M1_int):\n",
    "        if min(M2_int) <= M2_mod and M2_mod <= max(M2_int):\n",
    "            print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display models that lie within the range of observations for different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Count models\n",
    "import numpy as np\n",
    "data = np.load('CMIP5_sensitivity_nh.npy',allow_pickle=True)\n",
    "varidata = np.load('CMIP6_sensitivity_nh.npy',allow_pickle=True)\n",
    "data_obs = np.load('obs_sensitivity_nh.npy',allow_pickle=True)\n",
    "models = data['models']\n",
    "print(models)\n",
    "for metric in data_obs.keys():\n",
    "    print('****** '+metric+' ********')\n",
    "    number = 0\n",
    "    vari_mean = np.nanmean(varidata[metric+'_vari'])\n",
    "    obs_std = st_dev(data_obs[metric])\n",
    "    obs_metric = np.nanmean(data_obs[metric])\n",
    "    tolerance = 2*np.sqrt(vari_mean**2 + obs_std**2)\n",
    "    obs_interval = [obs_metric - tolerance,obs_metric + tolerance]\n",
    "    print(obs_interval)\n",
    "    print(obs_metric,tolerance)\n",
    "    for model in models:\n",
    "        #val = np.nanmean(data[metric+'_allmember'][model])\n",
    "        val = data[metric+'_allmember'][model][0]\n",
    "        #print('     '+model)\n",
    "        #print('     '+str(val))\n",
    "        if min(obs_interval) <= val and val <= max(obs_interval):\n",
    "            print(model)\n",
    "            #print(val)\n",
    "            number+=1\n",
    "    print(str(number)+'/'+str(len(models))+' models have nailed this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig. 1 (Sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This script plots Figure 2 and Figure 4 of the SIMIP paper draft. It reads the sensitivities from \n",
    "# npy files the sensitivities are calculated by the scripts sensitivity_cmip3.py, sensitivity_cmip5_6.py\n",
    "# and sensitivity_obs.py\n",
    "# Author: Jakob Doerr (jakob.doerr@mpimet.mpg.de)\n",
    "# Date: 25.09.2019\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "%matplotlib inline\n",
    "data = {} # data array for all the sensitivities\n",
    "cmips = ['CMIP3','CMIP5','CMIP6']\n",
    "hemisphere = 'nh'\n",
    "for cmip in cmips:\n",
    "    data[cmip] = np.load(cmip+'_sensitivity_'+hemisphere+'.npy',allow_pickle=True)\n",
    "# observed sensitivities\n",
    "obsdata = np.load('obs_sensitivity_'+hemisphere+'.npy',allow_pickle=True)\n",
    "# the metrics in figure 2\n",
    "metrics = ['sia_mean_mar_start','sia_mean_sept_start','sia_co2','sia_gmst','siv_mean_mar_start','siv_mean_sept_start']\n",
    "ylabels = {'sia_co2':'dSIA/dCO2 [m$^2$/t]','sia_gmst':'dSIA/dGMST [million km$^2$/째C]','gmst_co2':'dGMST/dCO2 [째C/1000Gt]',\n",
    "          'sia_mean_mar':'SIA March (1979-2014) [million km$^2$]', 'sia_mean_sept':'SIA Sept. (1979-2014) [million km$^2$]',\n",
    "          'siv_mean_mar':'SIV March (1979-2014) [10$^3$ km$^3$]','siv_mean_sept':'SIV Sept. (1979-2014) [10$^3$ km$^3$]',\n",
    "          'siv_mean_mar_start':'SIV March (1979-1998) [10$^3$ km$^3$]','siv_mean_sept_start':'SIV Sept. (1979-1998) [10$^3$ km$^3$]',\n",
    "          'piCtrl_var':'Sept. SIA variability [million km$^2$]','ECS':'Equilibrium climate sensitivity [째C]',\n",
    "          'sia_mean_mar_start':'SIA March (1979-1998) [million km$^2$]','sia_mean_sept_start':'SIA September (1979-1998) [million km$^2$]',\n",
    "          'sia_mean_mar_end':'SIA March (2005-2014) [million km$^2$]','sia_mean_sept_end':'SIA September (2005-2014) [million km$^2$]',\n",
    "          'sia_mean_sept_start_vari':'Var. of Sept. SIA (1979-1998)','sia_annual_cycle':'Annual cycle of SIA (2005-2014) [million km$^2$]'}\n",
    "titles = {'sia_co2':'dSIA/dCO2 (1979-2014)','sia_gmst':'dSIA/dGMST (1979-2014)','gmst_co2':'dGMST/dCO2 (1979-2014)',\n",
    "          'sia_mean_mar':'SIA (1979-2014)', 'sia_mean_sept':'SIA (1979-2014)',\n",
    "          'siv_mean_mar':'SIV (1979-2014)','siv_mean_sept':'SIV (1979-2014)',\n",
    "          'siv_mean_mar_start':'SIV (1979-1998)','siv_mean_sept_start':'SIV (1979-1998)',\n",
    "          'piCtrl_var':'Sept. SIA variability [million km$^2$]','ECS':'Equilibrium climate sensitivity [째C]',\n",
    "          'sia_mean_mar_start':'SIA (1979-1998)','sia_mean_sept_start':'SIA (1979-1998)',\n",
    "          'sia_mean_mar_end':'SIA (2005-2014)','sia_mean_sept_end':'SIA (2005-2014)',\n",
    "          'sia_mean_sept_start_vari':'Var. of Sept. SIA (1979-1998)','sia_annual_cycle':'Annual cycle of SIA (2005-2014)'}\n",
    "units = {'sia_co2':'[m$^2$/t]','sia_gmst':'[million km$^2$/째C]','gmst_co2':'[째C/1000Gt]',\n",
    "          'sia_mean_mar':'Sea-ice area [million km$^2$]', 'sia_mean_sept':'Sea-ice area [million km$^2$]',\n",
    "          'siv_mean_mar':'Sea-ice volume [thousand km$^3$]','siv_mean_sept':'Sea-ice volume [thousand km$^3$]',\n",
    "          'siv_mean_mar_start':'Sea-ice volume [thousand km$^3$]','siv_mean_sept_start':'Sea-ice volume [thousand km$^3$]',\n",
    "          'piCtrl_var':'[million km$^2$]','ECS':'[째C]',\n",
    "          'sia_mean_mar_start':'Sea-ice area [million km$^2$]','sia_mean_sept_start':'Sea-ice area [million km$^2$]',\n",
    "          'sia_mean_mar_end':'Sea-ice area [million km$^2$]','sia_mean_sept_end':'Sea-ice area [million km$^2$]',\n",
    "          'sia_mean_sept_start_vari':'Var. of Sept. SIA (1979-1998)','sia_annual_cycle':'Annual cycle of SIA (2005-2014) [million km$^2$]'}\n",
    "\n",
    "letters = ['a)','b)','c)','d)','e)','f)','g)','h)']\n",
    "colors = {'CMIP5':'#ff7f00','CMIP6':'green','CMIP3':'blue'}\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Figure 1 (comparison of metric for CMIP3, 5, 6)\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(11,8))\n",
    "i = 1\n",
    "for metric in metrics: # loop over metrics, a subplot for each metric\n",
    "    plt.subplot(2,4,i)\n",
    "    #plot observed metrics, where available\n",
    "    if metric in ['sia_co2','sia_gmst','gmst_co2','sia_mean_mar_start','sia_mean_sept_start']:\n",
    "        obs_mean = np.nanmean(obsdata[metric])\n",
    "        obs_std = st_dev(obsdata[metric])\n",
    "        plt.hlines(obs_mean,0,4,linestyle='--',color='k')\n",
    "        plt.hlines(obs_mean+obs_std,0,4,linestyle='dotted',color='k',linewidth=1)\n",
    "        plt.hlines(obs_mean-obs_std,0,4,linestyle='dotted',color='k',linewidth=1)\n",
    "        \n",
    "        vari_mean = np.nanmean(data['CMIP6'][metric+'_vari'])\n",
    "        plt.hlines(obs_mean+2*np.sqrt(vari_mean**2 + obs_std**2),0,4,linestyle='--',color='green',linewidth=1)\n",
    "        plt.hlines(obs_mean-2*np.sqrt(vari_mean**2 + obs_std**2),0,4,linestyle='--',color='green',linewidth=1)\n",
    "        # add shadings of the estimated internal variability for all CMIP6 single-model ensembles with 5 or more members\n",
    "        for vari in data['CMIP6'][metric+'_vari']:\n",
    "            plt.fill_between([0,4],[obs_mean-vari,obs_mean-vari],\n",
    "                             [obs_mean+vari,obs_mean+vari],facecolor='k',alpha=0.03)\n",
    "            \n",
    "    # plot the model metrics\n",
    "    for c,cmip in enumerate(cmips):\n",
    "        temp = data[cmip][metric].copy()\n",
    "        plt.plot(np.ones(len(data[cmip][metric]))*(c+1),temp,color=colors[cmip],linestyle='none',marker='_')\n",
    "        # call out the high sensitivity models\n",
    "        #if cmip == 'CMIP6':\n",
    "        #    plt.plot(np.ones(len(data[cmip][metric][data[cmip]['ECS']>=4.5]))*(c+1.14),temp[data[cmip]['ECS']>=4.5],\n",
    "        #             color='k',linestyle='none',marker='$<$',fillstyle='none',markeredgewidth=0.5,markersize=5)\n",
    "        # plot ensemble mean\n",
    "        plt.plot(c+1,np.nanmean(temp),color=colors[cmip],marker='x',markersize=10,linestyle='none')\n",
    "    plt.xticks([1,2,3],['CMIP3','CMIP5','CMIP6'])\n",
    "    plt.xlim(0.25,3.75)\n",
    "    plt.text(-0.3,1.02,letters[i-1],fontsize=12,transform=plt.gca().transAxes)\n",
    "    plt.ylabel(units[metric])\n",
    "    plt.title(titles[metric],fontsize=10,loc='center')\n",
    "    i+=1\n",
    "    \n",
    "    if metric == 'sia_mean_mar_start':\n",
    "        plt.ylim(9.5,22)\n",
    "    elif metric == 'sia_mean_sept_start':\n",
    "        plt.ylim(0,12.5)\n",
    "    if metric == 'siv_mean_mar_start':\n",
    "        plt.ylim(15,70)\n",
    "    elif metric == 'siv_mean_sept_start':\n",
    "        plt.ylim(0,55)\n",
    "print(data['CMIP6']['sia_gmst_allmember']['EC-Earth3-Veg'])\n",
    "\n",
    "###!!!!!!!!!!!!!!\n",
    "#data['CMIP5']['sia_co2'][-3] = np.nan\n",
    "###!!!!!!!!!!!!!!!!!!!\n",
    "# last panel: define function\n",
    "def plot_fig4(metric1,metric2,letter,cmips = ['CMIP3','CMIP5','CMIP6']):\n",
    "    plt.text(-0.12,1.02,letter,fontsize=12,transform=plt.gca().transAxes)\n",
    "    i = 0\n",
    "    for cmip in cmips:\n",
    "        data1 = data[cmip][metric1]\n",
    "        data2 = data[cmip][metric2]\n",
    "        mask = np.logical_and(~np.isnan(data1), ~np.isnan(data2))\n",
    "\n",
    "        f_data1 = data1[mask]\n",
    "        f_data2 = data2[mask]\n",
    "        plt.scatter(data1,data2,color=colors[cmip],s=30)\n",
    "        if len(f_data1) > 0:\n",
    "            corr = np.corrcoef(f_data1,f_data2)[1][0]\n",
    "            plt.text(0.74,0.95-i*0.045,cmip+': R = '+str(np.round(corr,2)),color=colors[cmip],transform=plt.gca().transAxes)\n",
    "            i += 1\n",
    "    # plot observations and internal variability around it:\n",
    "    if metric1 not in ['siv_mean_mar','siv_mean_mar_start','siv_mean_sept_start','piCtrl_var'] and metric2 not in ['siv_mean_mar_start','siv_mean_mar','siv_mean_sept','piCtrl_var']:\n",
    "        plt.scatter(np.nanmean(obsdata[metric1]),np.nanmean(obsdata[metric2]),color='black',s=50)\n",
    "        obs_std1 = st_dev(obsdata[metric1])\n",
    "        obs_std2 = st_dev(obsdata[metric2])\n",
    "        mean_x = np.nanmean(obsdata[metric1])\n",
    "        mean_y = np.nanmean(obsdata[metric2])\n",
    "        vari_x2 = 2*np.sqrt(np.nanmean(data['CMIP6'][metric1+'_vari'])**2+obs_std1**2)\n",
    "        vari_y2 = 2*np.sqrt(np.nanmean(data['CMIP6'][metric2+'_vari'])**2+obs_std2**2)\n",
    "        for i in range(len(data['CMIP6'][metric1+'_vari'])):\n",
    "            vari_x = data['CMIP6'][metric1+'_vari'][i]\n",
    "            vari_y = data['CMIP6'][metric2+'_vari'][i]\n",
    "            plt.fill_between([mean_x-vari_x,mean_x+vari_x],[mean_y-vari_y,mean_y-vari_y],\n",
    "                                     [mean_y+vari_y,mean_y+vari_y],facecolor='k',alpha=0.05)\n",
    "        plt.plot([mean_x-vari_x2,mean_x+vari_x2],[mean_y-vari_y2,mean_y-vari_y2],linestyle='--',color='green',linewidth=0.7)\n",
    "        plt.plot([mean_x-vari_x2,mean_x+vari_x2],[mean_y+vari_y2,mean_y+vari_y2],linestyle='--',color='green',linewidth=0.7)\n",
    "        plt.plot([mean_x+vari_x2,mean_x+vari_x2],[mean_y-vari_y2,mean_y+vari_y2],linestyle='--',color='green',linewidth=0.7)\n",
    "        plt.plot([mean_x-vari_x2,mean_x-vari_x2],[mean_y-vari_y2,mean_y+vari_y2],linestyle='--',color='green',linewidth=0.7)\n",
    "    plt.xlabel(ylabels[metric1])\n",
    "    plt.ylabel(ylabels[metric2])\n",
    "\n",
    "#plt.subplot(2,2,4)\n",
    "plt.axes([0.565, 0.06, 0.42, 0.384])\n",
    "plot_fig4('gmst_co2','sia_co2','g)')\n",
    "\n",
    "# add titles and vertical lines\n",
    "l1 = lines.Line2D([0.25, 0.25], [0, 1], transform=fig.transFigure, figure=fig, linestyle='--',color='black')\n",
    "fig.lines.extend([l1])\n",
    "fig.text(0.1,0.96,'March',fontsize=15)\n",
    "fig.text(0.6,0.96,'September',fontsize=15)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95],w_pad=2)\n",
    "plt.savefig('./plots/SIMIP_fig1.pdf')\n",
    "\n",
    "\n",
    "#### Plot mean area vs. SIA trends (CO2?)\n",
    "for cmip in cmips:\n",
    "    data[cmip]['SIA_trend'] = data[cmip]['sia_mean_sept_end'] - data[cmip]['sia_mean_sept_start']\n",
    "obsdata['SIA_trend'] = obsdata['sia_mean_sept_end'] - obsdata['sia_mean_sept_start']\n",
    "ylabels['SIA_trend'] = 'SIA (2005-2014) - (1979-1998)'\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "try:\n",
    "    plot_fig4('sia_mean_sept_start','SIA_trend','')\n",
    "except:\n",
    "    pass\n",
    "plt.ylabel('Sea-ice area loss: (2005-2014) - (1979-1998)\\n [million km$^2$]')\n",
    "plt.xlabel('Sea-ice area (1979-1998) [million km$^2$]')\n",
    "plt.savefig('mean_SIA_diff_SIA.pdf')\n",
    "#plt.tight_layout(rect=[0,0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig. 2 (Evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This script plots Figure 1 of the SIMIP paper draft. It reads the model data from mat files,\n",
    "# and the Observations (SIA, GMST) and the CO2 data from npy files.\n",
    "\n",
    "# Author: Jakob Doerr (jakob.doerr@mpimet.mpg.de\n",
    "# Date: 25.09.2019\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat,loadmat,whosmat\n",
    "from netCDF4 import Dataset,num2date\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from scipy import interpolate\n",
    "\n",
    "%matplotlib inline\n",
    "cmip = 'CMIP6'\n",
    "hemisphere = 'nh'\n",
    "\n",
    "co2path = '/work/mh0033/m300681/IPCC/processed_data/emissions/'\n",
    "cmippath = '/work/mh0033/m300681/IPCC/processed_data/'+cmip+'/'\n",
    "\n",
    "if cmip == 'CMIP6':\n",
    "    end_hist = 2014 # the final year of the historical scenario\n",
    "    scenarios = ['historical','ssp585','ssp245','ssp126']#,'ssp119']\n",
    "else:\n",
    "    end_hist = 2005\n",
    "    scenarios = ['historical','rcp26','rcp45','rcp85']\n",
    "\n",
    "# create the time vectors\n",
    "# model time\n",
    "years = {'historical':np.arange(1950,end_hist+1),'ssp126':np.arange(2015,2100),'ssp119':np.arange(2015,2100),\n",
    "         'ssp245':np.arange(2015,2100),'ssp585':np.arange(2015,2100), 'rcp26':np.arange(2006,2100),\n",
    "        'rcp85':np.arange(2006,2100),'rcp45':np.arange(2006,2100)}\n",
    "# observations time\n",
    "obs_years = np.arange(1979,2019)\n",
    "\n",
    "# colors for the scenarios (from https://pyam-iamc.readthedocs.io/en/latest/tutorials/ipcc_colors.html)\n",
    "colors = {'historical':'grey','ssp245':'#a18e1a','ssp126':'#003466', 'ssp585':'#990002',\n",
    "         'rcp85':'#990002','rcp45':'#5492CD','rcp26':(0,52/255,102/255),'ssp119':'#00AAD0'}\n",
    "letters = ['a)','b)','c)','d)','e)','f)','g)','h)']\n",
    "month_names = {3:'March',9:'September'}\n",
    "ylims = {3:[0,22],9:[0,12]}\n",
    "names = {'historical':'Historical','ssp119':'SSP1-1.9','ssp126':'SSP1-2.6','ssp245':'SSP2-4.5','ssp585':'SSP5-8.5'}\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "## Step 1: Read in the relevant data\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# cmip6 data\n",
    "# the data is stored in mat files, which contain data arrays with the dimensions (simulation, years, months)\n",
    "# for SIA, SIV and GMST (no month dimension for GMST)\n",
    "# the simulation dimension has an entry for every model member and the list of simulations is also stored in \n",
    "# the mat file, in the format 'MODEL-NAME_r*i1p1f*'\n",
    "\n",
    "# determine the models which have SIA or GMST for any scenario. Later, once a model has no data, it will\n",
    "# not be drawn in the figure (because of NaNs)\n",
    "modellist = []\n",
    "for scenario in scenarios:\n",
    "    data = loadmat(cmip+'_'+scenario+'_'+hemisphere+'.mat')\n",
    "    m_list = list(set([i[0:i.find('_')] for i in data['Simulation_names']]))\n",
    "    modellist = np.concatenate([modellist,m_list])\n",
    "modellist = list(sorted(set(modellist)))\n",
    "print(modellist)\n",
    "\n",
    "# create data arrays \n",
    "sia_cmip6 = {3:{},9:{}} # march and september for SIA and SIV\n",
    "siv_cmip6 = {3:{},9:{}}\n",
    "gmst_cmip6 = {} # yearly for GMST\n",
    "\n",
    "# Start reading in the model data and pick the first member of every model\n",
    "for scenario in scenarios:\n",
    "    print(scenario)\n",
    "    #Initialize data arrays:\n",
    "    for month in [3,9]:\n",
    "        sia_cmip6[month][scenario] = np.zeros((len(modellist),len(years[scenario])))\n",
    "        siv_cmip6[month][scenario] = np.zeros((len(modellist),len(years[scenario])))\n",
    "    gmst_cmip6[scenario] = np.zeros((len(modellist),len(years[scenario])))  \n",
    "    \n",
    "    # read in data\n",
    "    raw_data = loadmat(cmip+'_'+scenario+'_'+hemisphere+'.mat')\n",
    "    raw_years = raw_data['Years'][0]\n",
    "    simulations = raw_data['Simulation_names']\n",
    "    \n",
    "    # create an index array which maps the years of the scenario in this script to the years in \n",
    "    # the scenario in the .mat data.\n",
    "    year_idx = np.where(np.isin(raw_years,years[scenario]))[0]\n",
    "    # create an index array for the control period, to be able to compute GMST anomalies\n",
    "    control_idx = np.where(np.isin(raw_years,np.arange(1850,1900)))[0] \n",
    "    \n",
    "    # sort data to data arrays and select first member\n",
    "    for i,model in enumerate(modellist):\n",
    "        index = np.asarray([j for j in range(len(simulations)) if model+'_' in simulations[j]])\n",
    "        if len(index) > 0: # if there is data for this model in the mat file\n",
    "            # sia, siv\n",
    "            for month in [3,9]:\n",
    "                # sia\n",
    "                curr_data = raw_data['sia_'+hemisphere][index,:,month-1]\n",
    "                sia_cmip6[month][scenario][i,:] = curr_data[0,year_idx]\n",
    "                #sia_cmip6[month][scenario][i,:] = np.nanmean(curr_data[:,year_idx],0)\n",
    "                # siv\n",
    "                curr_data = raw_data['siv_'+hemisphere][index,:,month-1]\n",
    "                siv_cmip6[month][scenario][i,:] = curr_data[0,year_idx]\n",
    "                \n",
    "                #siv_cmip6[month][scenario][i,:] = np.nanmean(curr_data[:,year_idx],0)\n",
    "            # gmst (calculate anomalies) \n",
    "            curr_data = raw_data['gmst'][index,:]\n",
    "            gmst_cmip6[scenario][i,:] = curr_data[0,year_idx] - np.nanmean(curr_data[0,control_idx])\n",
    "            #gmst_cmip6[scenario][i,:] = np.nanmean(curr_data[:,year_idx] - np.nanmean(curr_data[:,control_idx]),0)\n",
    "        else: # if there is no data, fill with NaNs\n",
    "            for month in [3,9]:\n",
    "                sia_cmip6[month][scenario][i,:] *= np.nan\n",
    "                siv_cmip6[month][scenario][i,:] *= np.nan\n",
    "            gmst_cmip6[scenario][i,:] *= np.nan\n",
    "# ---------------------\n",
    "# observations of SIA, take mean over several\n",
    "# the observations are saved in npy files\n",
    "obs_names = ['osisaf','nsidc_nt','nsidc_bt']\n",
    "obspath = '/home/mpim/m300681/SIA_paper/data/'\n",
    "obs_sia = {}\n",
    "data = np.load(obspath+'SIA_observations_'+hemisphere+'.npy',allow_pickle=True)\n",
    "obs_time = data['years']\n",
    "for month in [3,9]:\n",
    "    obs_data = np.nanmean([data[obs_name][\"{:02d}\".format(month)] for obs_name in obs_names],0)\n",
    "    obs_sia[month] = np.zeros(len(obs_years))*np.nan\n",
    "    # map the data to the correct years\n",
    "    obs_sia[month][np.isin(obs_years,obs_time)] = obs_data[np.isin(obs_time,obs_years)]\n",
    "    \n",
    "# ---------------------\n",
    "# observations of GMST \n",
    "obs_names = ['noaa','gistemp','HadCRUT','berkeley']\n",
    "obs_gmst = np.zeros((len(obs_names),len(obs_years)))*np.nan\n",
    "for i,obs_name in enumerate(obs_names):\n",
    "    gmstfile = Dataset('/work/mh0033/m300681/IPCC/processed_data/obs/GMST_'+obs_name+'_yearly.nc')\n",
    "    obs_data = np.squeeze(gmstfile.variables['tas_ano'][:].data.copy()) \n",
    "    time = np.squeeze(num2date(gmstfile.variables['time'][:].copy(),\n",
    "                           gmstfile.variables[\"time\"].units,\n",
    "                           calendar = gmstfile.variables[\"time\"].calendar))\n",
    "    raw_years = np.asarray([time[i].year for i in range(len(time))])\n",
    "    # the control period index array for the anomalies:\n",
    "    control_idx = np.where(np.isin(raw_years,np.arange(1850,1900)))\n",
    "\n",
    "    obs_data = obs_data - np.nanmean(obs_data[control_idx])\n",
    "    obs_gmst[i,:] = np.zeros(len(obs_years))*np.nan\n",
    "    # map the data to the correct years\n",
    "    obs_gmst[i,np.isin(obs_years,raw_years)] = obs_data[np.isin(raw_years,obs_years)]\n",
    "obs_gmst = np.nanmean(obs_gmst,0) \n",
    "# --------------------\n",
    "# read in the cumulative co2 emissions for the simulations and for the observations\n",
    "# they are also stored in npy files\n",
    "# for the historical period until 2017, emissions are taken from the Carbon Budget 2018 \n",
    "# (https://www.earth-syst-sci-data.net/10/2141/2018/)\n",
    "# for the scenarios, CMIP6 co2 data is taken from the Input4MIP emissions data \n",
    "# (https://tntcat.iiasa.ac.at/SspDb/dsd?Action=htmlpage&page=welcome)\n",
    "co2 = {}\n",
    "data = np.load(co2path+'CO2_'+cmip+'.npy',allow_pickle=True)\n",
    "#data = np.load(co2path+'CO2_conc_'+cmip+'.npy',allow_pickle=True)\n",
    "for scenario in scenarios:   \n",
    "    co2data = data[scenario][1,:]\n",
    "    co2years = data[scenario][0,:]\n",
    "    co2[scenario] = co2data[np.isin(co2years,years[scenario])]\n",
    "    # for the co2 emissions in the period of observations, use emissions data for ssp245, \n",
    "    # because observed co2 emissions only go until 2017 (but sia and gmst observations until 2018)\n",
    "    if scenario == 'ssp245':\n",
    "        # co2 data for observations\n",
    "        obs_co2 = co2data[np.isin(co2years,obs_years)]\n",
    "        ## co2 for satellite historical period (1979-2014)\n",
    "        hist_co2 = co2data[np.isin(co2years,np.arange(1979,2015))]\n",
    "\n",
    "        \n",
    "# end of data read in \n",
    "# ------------------------------------------------------------------------------------------\n",
    "# plot \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "# draw SIA vs CO2 emissions\n",
    "for m,month in enumerate([3,9]):\n",
    "    plt.subplot(2,3,m*3+1)\n",
    "    plt.text(-0.24,1.0,letters[m*3],fontsize=12,transform=plt.gca().transAxes)\n",
    "    for scenario in scenarios:\n",
    "        # sort the co2 values, b/c there can be a decrease in the cumulative co2 emissions\n",
    "        # (for example in ssp126), which will look ugly in the plot\n",
    "        x = co2[scenario]\n",
    "        isort = np.argsort(x)\n",
    "        x = x[isort]\n",
    "        y_mean = np.nanmean(sia_cmip6[month][scenario],0)[isort]\n",
    "        y_std = np.nanstd(sia_cmip6[month][scenario],0,ddof=1)[isort]\n",
    "        \n",
    "        for i,model in enumerate(modellist):\n",
    "            plt.plot(co2[scenario],sia_cmip6[month][scenario][i,:],color=colors[scenario],\n",
    "                     marker='.',alpha=0.1,linestyle='',markersize=1.414)\n",
    "        # multi-model mean\n",
    "        plt.plot(x,y_mean,color=colors[scenario],linewidth=2,zorder=100)\n",
    "        # shading around mean\n",
    "        plt.fill_between(x,y_mean-y_std,y_mean+y_std,facecolor=colors[scenario],alpha=0.3,zorder=100)\n",
    "    # observations\n",
    "    plt.plot(obs_co2,obs_sia[month],color='k',linestyle='-',marker='',markersize=np.sqrt(5),zorder=101) \n",
    "    # add 1 million km짼 line for ice-free threshold\n",
    "    if month == 9:\n",
    "        plt.plot([0,10000],[1,1],linestyle='--',color='k',linewidth=0.5)\n",
    "    plt.ylabel('Sea ice area [million km$^2$]')\n",
    "    plt.ylim(ylims[month])\n",
    "    plt.xlim(0,10000)\n",
    "    plt.gca().spines['right'].set_color('none')\n",
    "    plt.gca().spines['top'].set_color('none')\n",
    "plt.xlabel('Cumulative CO2 emissions [Gt]') \n",
    "\n",
    "# draw SIA vs GMST for March and September\n",
    "for m,month in enumerate([3,9]):\n",
    "    ax = plt.subplot(2,3,m*3+2)\n",
    "    ax.text(-0.24,1.0,letters[m*3+1],fontsize=12,transform=plt.gca().transAxes)\n",
    "    for scenario in scenarios:\n",
    "        for i,model in enumerate(modellist):\n",
    "            ax.plot(gmst_cmip6[scenario][i,:],sia_cmip6[month][scenario][i,:],\n",
    "                    color=colors[scenario],marker='.',alpha=0.1,linestyle='',markersize=1.414)\n",
    "            \n",
    "        # try to draw a 'model mean' and a shading around the data, which is difficult,\n",
    "        # since both GMST and SIA are varying between models\n",
    "        # to do so, bin the data in GMST bins and compute mean and STD for every GMST bin (does not work too well, though, see figures)\n",
    "        if scenario == 'historical':\n",
    "            bins = np.linspace(np.nanpercentile(gmst_cmip6[scenario],5),min(np.nanpercentile(gmst_cmip6[scenario][:,-5::],99),5),10)\n",
    "        else:\n",
    "            bins = np.linspace(np.nanpercentile(gmst_cmip6[scenario],5),min(np.nanpercentile(gmst_cmip6[scenario][:,-5::],90),5),10)\n",
    "        binned_index = np.digitize(gmst_cmip6[scenario],bins)\n",
    "        binned_gmst_mean = np.asarray([np.nanmedian(gmst_cmip6[scenario][binned_index == i]) \n",
    "                                       for i in range(1, len(bins)) if len(binned_index[binned_index == i])> 50])\n",
    "        binned_sia_mean = np.asarray([np.nanmedian(sia_cmip6[month][scenario][binned_index == i]) \n",
    "                                      for i in range(1, len(bins)) if len(binned_index[binned_index == i])> 50])\n",
    "        binned_sia_std = [np.nanstd(sia_cmip6[month][scenario][binned_index == i],ddof=1) \n",
    "                          for i in range(1, len(bins)) if len(binned_index[binned_index == i])> 50]\n",
    "        # bin mean\n",
    "        ax.plot(binned_gmst_mean,binned_sia_mean,color=colors[scenario],linewidth=2,label=names[scenario],zorder=100)\n",
    "        lower_bound = np.max([binned_sia_mean*0,binned_sia_mean-binned_sia_std],0)\n",
    "        upper_bound = binned_sia_mean+binned_sia_std\n",
    "        # bin shading\n",
    "        ax.fill_between(binned_gmst_mean,lower_bound,upper_bound,facecolor=colors[scenario],alpha=0.3,zorder=100)\n",
    "     \n",
    "        #X = gmst_cmip6[scenario].flatten()\n",
    "        #Y = sia_cmip6[month][scenario].flatten()\n",
    "        ##ys = lowess(Y,X)\n",
    "        ##plt.plot(ys[:,0],ys[:,1],linewidth=2,color=colors[scenario],label=names[scenario])\n",
    "        #mask = np.where(np.logical_and(~np.isnan(X),~np.isnan(Y)))\n",
    "        #X = X[mask]\n",
    "        #Y = Y[mask]\n",
    "        #confidence_ellipse(X,Y,ax,facecolor=colors[scenario])\n",
    "    # finally, add observations to the plot\n",
    "    ax.plot(obs_gmst,obs_sia[month],color='k',linestyle='',marker='x',markersize=np.sqrt(10),label='Observations',zorder=101)   \n",
    "    # add 1 million km짼 line for ice-free threshold\n",
    "    if month == 9:\n",
    "        plt.plot([0,10],[1,1],linestyle='--',color='k',linewidth=0.5)\n",
    "    #ax.set_ylabel('Sea ice area [million km$^2$]')\n",
    "    ax.set_xlim(0,5)\n",
    "    ax.set_ylim(ylims[month])\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.set_xticks([0,1,2,3,4,5])\n",
    "    ax.set_xticklabels(['0','1','2','3','4','5'])\n",
    "    ax.set_xticks( [1,1.5,3,5], minor=True )\n",
    "ax.set_xlabel('Surface temperature change [째C]')\n",
    "lgd = plt.legend(frameon=False,labelspacing=0.2,markerscale=0,loc='upper right')\n",
    "for item in lgd.legendHandles:\n",
    "    item.set_visible(False)\n",
    "# set colors of legend\n",
    "for i,text in enumerate(lgd.get_texts()):\n",
    "    plt.setp(text, color = lgd.legendHandles[i].get_c(),weight='bold')    \n",
    "\n",
    "# draw SIA vs time\n",
    "for m,month in enumerate([3,9]):\n",
    "    ax = plt.subplot(2,3,m*3+3)\n",
    "    ax.text(-0.24,1.0,letters[m*3+2],fontsize=12,transform=plt.gca().transAxes)\n",
    "    #ax_lines = plt.subplot(2,20,14+m)\n",
    "    for j,scenario in enumerate(scenarios):\n",
    "        for i,model in enumerate(modellist):\n",
    "            ax.plot(years[scenario],sia_cmip6[month][scenario][i,:],color=colors[scenario],\n",
    "                    marker='.',alpha=0.1,linestyle='',markersize=1.414)\n",
    "        # multi-model mean\n",
    "        ax.plot(years[scenario],np.nanmean(sia_cmip6[month][scenario],0),color=colors[scenario],linewidth=2,zorder=100)\n",
    "        lower_bound = np.nanmean(sia_cmip6[month][scenario],0)-np.nanstd(sia_cmip6[month][scenario],0,ddof=1)\n",
    "        upper_bound = np.nanmean(sia_cmip6[month][scenario],0)+np.nanstd(sia_cmip6[month][scenario],0,ddof=1)\n",
    "        # shadig around mean\n",
    "        ax.fill_between(years[scenario],lower_bound,upper_bound,facecolor=colors[scenario],alpha=0.3,zorder=100)\n",
    "        \n",
    "        # add vertical lines at the end for future scenarios\n",
    "        if scenario != 'historical':\n",
    "            rect = patches.Rectangle((2100+3*j,max(lower_bound[-1],0)),1,upper_bound[-1]-max(lower_bound[-1],0),\n",
    "                                     edgecolor=colors[scenario],facecolor=colors[scenario],clip_on=False)\n",
    "            #ax.add_patch(rect)\n",
    "    # observations\n",
    "    ax.plot(obs_years,obs_sia[month],color='k',linestyle='-',marker='',markersize=np.sqrt(5),zorder=101)\n",
    "    # add 1 million km짼 line for ice-free threshold\n",
    "    if month == 9:\n",
    "        plt.plot([1950,2100],[1,1],linestyle='--',color='k',linewidth=0.5)\n",
    "    #ax.set_ylabel('Sea ice area [million km$^2$]')\n",
    "    ax.set_ylim(ylims[month])\n",
    "    ax.set_xlim(1950,2100)\n",
    "\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "ax.set_xlabel('Year')\n",
    "\n",
    "fig.text(0,0.96,'March',transform=fig.transFigure,fontsize=15)\n",
    "fig.text(0,0.48,'September',transform=fig.transFigure,fontsize=15)\n",
    "plt.tight_layout(h_pad = 4,w_pad = 0.5,rect=[0, 0, 1, 0.95])\n",
    "plt.savefig('./plots/SIMIP_fig3.pdf')\n",
    "\n",
    "# end of script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig. 3 (Ice free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "colors = {'historical':'grey','ssp245':'#a18e1a','ssp126':'#003466', 'ssp585':'#990002',\n",
    "         'rcp85':'#990002','rcp45':'#5492CD','rcp26':(0,52/255,102/255),'ssp119':'#00AAD0'}\n",
    "data = {} # data array for all the sensitivities\n",
    "names = {'historical':'Historical','ssp119':'SSP1-1.9','ssp126':'SSP1-2.6','ssp245':'SSP2-4.5','ssp585':'SSP5-8.5'}\n",
    "ylabels = {'time':'Year','co2':'Future cumulative \\nCO2 emissions [Gt]','temp':'Surface temperature\\n anomaly [째C]'}\n",
    "ticks = {'co2':[0,1000,2000,3000,4000,5000,6000,7000,8000],'time':[2020,2030,2040,2050,2060,2070,2080,2090,2100],\n",
    "        'temp':[1.5,2,2.5,3,3.5]}\n",
    "minorticks = {'co2':[1000,3000,5000,7000,8000],'time':[2010,2030,2050,2070,2090],\n",
    "        'temp':[1.5,2,2.5,3,3.5]}\n",
    "ylim = {'time':2100,'co2':8000,'temp':3.5}\n",
    "letters = ['a)','b)','c)']\n",
    "hemisphere = 'nh'\n",
    "plt.figure(figsize=(9,3))\n",
    "for o,ordinate in enumerate(['co2','temp','time']):\n",
    "    plt.subplot(1,3,o+1)\n",
    "    data[ordinate] = {}\n",
    "    for m,mode in enumerate(['all','select']):\n",
    "        temp = np.load('CMIP6_ice_free_'+mode+'_'+ordinate+'.npy',allow_pickle=True)\n",
    "        for s,scenario in enumerate(['ssp119','ssp126','ssp245','ssp585']):\n",
    "            temp_data = np.asarray(temp[scenario])\n",
    "            gets_icefree = np.asarray(np.load('CMIP6_ice_free_'+mode+'_time.npy',allow_pickle=True)[scenario])\n",
    "            n_stillice = len(gets_icefree[np.logical_and(gets_icefree>2100,~np.isnan(temp_data))])\n",
    "            n_members = len(gets_icefree[~np.isnan(gets_icefree)])\n",
    "            if ordinate == 'co2':\n",
    "                temp_data = temp_data - 2400\n",
    "                print(mode)\n",
    "                print(len(temp_data[temp_data<1000]))\n",
    "            temp_data = temp_data[gets_icefree<2101]\n",
    "            if mode == 'all':\n",
    "                plt.plot(np.ones(len(temp_data))*(m+1)+0.2*s-0.3,temp_data,\n",
    "                     color=colors[scenario],linestyle='none',marker='o',alpha=0.4,markersize=2,label=names[scenario])\n",
    "            else:\n",
    "                plt.plot(np.ones(len(temp_data))*(m+1)+0.2*s-0.3,temp_data,\n",
    "                     color=colors[scenario],linestyle='none',marker='o',alpha=0.4,markersize=2)\n",
    "            plt.text((m)*0.5+0.1*s+0.09,1.06,str(n_stillice),fontsize=8,transform=plt.gca().transAxes,horizontalalignment='center')\n",
    "            plt.text((m)*0.5+0.1*s+0.09,1.01,str(n_members),fontsize=8,transform=plt.gca().transAxes,horizontalalignment='center')\n",
    "    plt.xticks([1,2,3,4],['All\\nmodels','Selected\\nmodels']) \n",
    "    plt.xlim(0.5,2.5)\n",
    "    plt.ylim(top=ylim[ordinate])\n",
    "    plt.ylabel(ylabels[ordinate])\n",
    "    plt.text(-0.25,1.07,letters[o],transform = plt.gca().transAxes)\n",
    "    #plt.yticks(ticks[ordinate])\n",
    "    plt.gca().set_yticks( minorticks[ordinate], minor=True )\n",
    "    if ordinate == 'co2':\n",
    "        lgd = plt.legend(frameon=False,labelspacing=0.2,markerscale=0,fontsize=8,loc='upper left',bbox_to_anchor=[-0.15,0,1,1])\n",
    "        for item in lgd.legendHandles:\n",
    "            item.set_visible(False)\n",
    "        # set colors of legend\n",
    "        for i,text in enumerate(lgd.get_texts()):\n",
    "            plt.setp(text, color = lgd.legendHandles[i].get_c(),weight='bold')  \n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.savefig('./plots/New_fig4.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SI: Plot 20-yr mean of GMST observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset, num2date\n",
    "\n",
    "# observations of GMST \n",
    "obs_years = np.arange(1850,2019)\n",
    "obs_names = ['noaa','gistemp','HadCRUT','berkeley']\n",
    "obs_gmst = np.zeros((len(obs_names),len(obs_years)))*np.nan\n",
    "for i,obs_name in enumerate(obs_names):\n",
    "    gmstfile = Dataset('/work/mh0033/m300681/IPCC/processed_data/obs/GMST_'+obs_name+'_yearly.nc')\n",
    "    obs_data = np.squeeze(gmstfile.variables['tas_ano'][:].data.copy()) \n",
    "    time = np.squeeze(num2date(gmstfile.variables['time'][:].copy(),\n",
    "                           gmstfile.variables[\"time\"].units,\n",
    "                           calendar = gmstfile.variables[\"time\"].calendar))\n",
    "    raw_years = np.asarray([time[i].year for i in range(len(time))])\n",
    "    print(raw_years[0])\n",
    "    # the control period index array for the anomalies:\n",
    "    control_idx = np.where(np.isin(raw_years,np.arange(1850,1900)))\n",
    "\n",
    "    obs_data = obs_data - np.nanmean(obs_data[control_idx])\n",
    "    obs_gmst[i,:] = np.zeros(len(obs_years))*np.nan\n",
    "    # map the data to the correct years\n",
    "    obs_gmst[i,np.isin(obs_years,raw_years)] = obs_data[np.isin(raw_years,obs_years)]\n",
    "    \n",
    "for i in range(4):\n",
    "    plt.plot(obs_years[9:-10],np.convolve(obs_gmst[i,:], np.ones((20,))/20, mode='valid'),label=obs_names[i].upper())\n",
    "plt.legend()\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Temp. anomaly vs. 1850-1900 [째C]')\n",
    "plt.savefig('gmst_obs_20yr_mean.pdf')\n",
    "plt.figure()\n",
    "for i in range(4):\n",
    "    plt.plot(obs_years,obs_gmst[i,:],label=obs_names[i].upper())\n",
    "plt.legend()\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Temp. anomaly vs. 1850-1900 [째C]')\n",
    "plt.savefig('gmst_obs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
